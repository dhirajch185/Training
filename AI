Local llm

Step1 : install python 3 and ollama service on the machine.
step2: python3 -m venv venv --> Next navigate to scripts folder in venv and activate
step3: pip install ollama
step4: write python code to send the prompt like below

---------------------------------------- ollama library based example ---------------------------------
import ollama

PROMPT = """
ONLY Generate an ideal Dockerfile for {language} with best practices. Do not provide any description
Include:
- Base image
- Installing dependencies
- Setting working directory
- Adding source code
- Running the application
- Multi stage docker build
"""

def generate_dockerfile(language):
    response = ollama.chat(model='llama3.2:1b', messages=[{'role': 'user', 'content': PROMPT.format(language=language)}])
    return response['message']['content']

if __name__ == '__main__':
    language = input("Enter the programming language: ")
    dockerfile = generate_dockerfile(language)
    print("\nGenerated Dockerfile:\n")
    print(dockerfile)

---------------------------------- with gemini model ---------------------------------

import google.generativeai as genai
import os

# Set your API key here
os.environ["GOOGLE_API_KEY"] = "****************My Google API Key*************"

# Configure the Gemini model
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-pro')

PROMPT = """
Generate an ideal Dockerfile for {language} with best practices. Just share the dockerfile without any explanation between two lines to make copying dockerfile easy.
Include:
- Base image
- Installing dependencies
- Setting working directory
- Adding source code
- Running the application
- Multi stage docker build
"""

def generate_dockerfile(language):
    response = model.generate_content(PROMPT.format(language=language))
    return response.text

if __name__ == '__main__':
    language = input("Enter the programming language: ")
    dockerfile = generate_dockerfile(language)
    print("\nGenerated Dockerfile:\n")
    print(dockerfile)

-------------------------------------------------------------------------------------------------

Gen AI

- Computing power access made AI possible and accessible to most of us.
What’s the difference between AI, ML and gen AI?

** AI - Building machines or systems that can do tasks what requires humans for it. like learning, problem solving, decision making.

** ML - A subset AI. Its all about using data to train machines to perform specific tasks. AI systems uses Math ( A system of euqtions) to solve these problems in ML models. Based on data we input, we get different output. So data plays a huge role.

** GenAI (Generative AI) - Its a subset of ML. It focuses on creating new content like images, audio, text. It generates someting entirely new. It learns underlying patterns and structures of the data they are trained on. Use what they learned and genreate new content. To learn these patterns well, these need a lot of data.

Machine learning models predict the future based on existing data, much like how humans use experience to make educated guesses. However, while humans might rely on intuition or gut feelings, these models use probability.
Data can be - Numbers (temperature, date), Text description (description of an item's appearance),Images or sounds. Depending on what you want your ML model to do, some data is relevant and some is irrelevant.

Data Quality depends on --> Accuracy, Size of the dataset, Representative (Data needs to be representative and inclusive, otherwise it can lead to skewed samples and biased outcomes. If a dataset about customer preferences is missing information about a certain demographic, the model might make inaccurate or biased generalizations about that group.), consistency (  Inconsistent data formats or labeling can confuse the model and hinder its ability to learn effectively. Imagine trying to assemble a puzzle where some pieces are labeled with numbers and others with letters—it would be a mess.), Relevance ( The data must be relevant to the task the AI is designed to perform. For example, data about traffic patterns in London is unlikely to be helpful for predicting crop yields in Banglore.

Data Accessibility - The ability of AI systems to effectively utilize this data is directly tied to data accessibility 
        ** Availability - If the necessary data simply isn't available, the AI model can't be trained. For some problems, the data might exist, but it might be locked behind paywalls or restricted due to privacy concerns.
        ** Cost - Gathering and cleaning data can be expensive. The cost of acquiring high-quality data can be a significant barrier to AI development, especially for smaller organizations.
    ** Format - Data needs to be in a format that the AI model can understand and process. Converting data into the appropriate format can be time-consuming and technically challenging.

Types of Data: Structured (organized in tables, mostly from a relational DB) and unstructured (pdf, emails, social media posts, images,audio and video).

Machine learning - 3 primary learning approaches; each with its own data requirements
                                                ---> supervised - Supervised models rely on labeled data
                                                    unsupervised - unsupervised models work with unlabeled data
                                                    reinforcement learning - reinforcement learning learns through interaction and feedback
Labeled Data - An image dataset for training a cat-detection model would label each picture as either a cat or dog. Similarly, a set of customer reviews might be labeled as positive, negative, or neutral. These labels enable algorithms to learn relationships and make accurate predictions.

Unlabeled Data - A collection of unorganized photos, a stream of audio recordings, or website traffic logs without user categorization. In these cases, algorithms must independently discover patterns and structures within the data, as there are no pre-existing labels to guide the learning process.

Supervised Machine Learning - Train models on labeled data, where each input is paired with its correct output, allowing the model to learn the relationship between them. The model's goal is to identify patterns and relationships within this labeled data, enabling it to accurately predict outputs for new, unseen inputs.
Example------->Predicting housing prices is a common example of supervised learning. A model is trained on a dataset where each house has labeled data, such as its size, number of bedrooms, location, and the corresponding sale price. This labeled data allows the algorithm to learn the relationship between the features of a house and its price. Once trained, the model can then predict the price of a new house based on its features.

Unsupervised Machine Learning - ML model deal with raw, unlabeled data to find natural groupings. Instead of learning from labeled data, it dives headfirst into a sea of unlabeled data.
Example-------> An unsupervised learning algorithm could analyze customer purchase history from a company's database. It might uncover hidden segments of customers with similar buying habits, even though you never explicitly labeled those segments beforehand. This can be incredibly valuable for targeted marketing or product recommendations.

Think of it as exploratory analysis. Unsupervised learning helps you understand the underlying structure of your data and uncover insights that you might not have even known to look for.

Reinforcement learning  --> learning through interaction and feedback. Imagine a robot learning to navigate a maze. It starts with no knowledge of the maze's layout. As it explores and interacts with the maze, it collects data—bumping into walls (negative feedback) or finding shortcuts (positive feedback). Through this process of trial and error, the algorithm learns which actions lead to the best outcomes. 
In reinforcement learning, the algorithm learns to maximize rewards and minimize penalties by interacting with its environment.
This type of learning is particularly useful in situations where you can't provide explicit instructions or labeled data. For example, you could use reinforcement learning to train a self-driving car to navigate complex traffic situations or to optimize the performance of a robot in a manufacturing plant.

------------->
Gather data by ingestion --> Pub/Sub handles real-time streaming data processing, regardless of the structure of the data. 
                            Cloud Storage is well-suited for storing unstructured data.
                            Cloud SQL and Cloud Spanner are used to manage structured data.

Data preparation -----> Its the process of cleaning and transforming raw data into a usable format for analysis or model training. This involves formatting and labeling data properly. 
                        Google Cloud offers tools like BigQuery for data analysis and Data Catalog for data governance. These tools help prepare data for ML models. 
                        With BigQuery, you can filter data, correct its inconsistencies, and handle missing values. 
                        With Data Catalog, you can find relevant data for your ML projects. This tool provides a centralized repository to easily discover datasets in your organization.

Train your Model -----> The process of creating your ML model using data is called model training. Google Cloud's Vertex AI platform provides a managed environment for training ML models. 
                        With Vertex AI, you can set parameters and build your model, using prebuilt containers for popular machine learning frameworks, custom training jobs, and tools for model evaluation.
                        Vertex AI also provides access to powerful computing resources to make the model training process faster.

Deploy & Predict ----> Model deployment is the process of making a trained model available for use. 

                        Vertex AI simplifies this by providing tools to put the model into action for generating predictions. This includes scaling the deployment, which means adjusting the resources allocated to the model to handle varying amounts of usage.

Model Management ----> Model management is the process of managing and maintaining your models over time. Google Cloud offers tools for managing the entire lifecycle of ML models. This includes the following:

                        Versioning: Keep track of different versions of the model.
                        Performance tracking: Review the model metrics to check the model's performance.
                        Drift monitoring: Watch for changes in the model's accuracy over time.
                        Data management: Use Vertex AI Feature Store to manage the data features the model uses.
                        Storage: Use Vertex AI Model Garden to store and organize the models in one place.
                        Automate: Use Vertex AI Pipelines to automate your machine learning tasks.


------------------------------------------------------

Deep Learning - A powerful subset of machine learning, distinguished by its use of artificial neural networks. These networks enable the processing of highly complex patterns and the generation of sophisticated predictions.

Neural networks can leverage both labeled and unlabeled data, a strategy known as semi-supervised learning. They train on a blend of a small amount of labeled data and a large amount of unlabeled data. That way, they learn foundational concepts and generalize effectively to novel (new, not previously experienced)  examples.

Large language models (LLMs)​​​

One particularly exciting type of foundation model is the LLM. These models are specifically designed to understand and generate human language. 

They can translate languages, write different kinds of creative content, and answer your questions in an informative way, even if they are open ended, challenging, or strange. This is likely the most common foundation model you've encountered, such as in popular generative AI chatbots like Gemini. They also help power many search engines you use today.

Diffusion models
Diffusion models are another type of foundational model. They excel in generating high-quality images, audio, and even video by iteratively refining noise (or unstructured/random data and patterns) into structured data.

To summarize, deep learning provides the core technology, foundation models are the powerful architectures built on deep learning, and generative AI is the application of these models to create new and original content.


--------------------------------------------------------------

Modality -  Modality refers to the type of data the model can process and generate, such as text, images, video, or audio. If your application focuses on a single data type, like generating text-based articles or creating audio files, you'll want to choose a model optimized for that specific modality.

context window - The amount of information a model can consider at one time when generating a response. A larger context window allows the model to "remember" more of the conversation or document, leading to more coherent and relevant outputs, especially for longer texts or complex tasks.  larger context windows often come with increased computational costs. You need to balance the need for context with the practical limitations of your resources.

Security -  Consider the model's security features, including data encryption, access controls, and vulnerability management. Ensure the model complies with relevant security standards and regulations for your industry.

Availability and reliability - Choose a model that is consistently available and performs reliably under load. Consider factors like uptime guarantees, redundancy, and disaster recovery mechanisms.

Cost - 
