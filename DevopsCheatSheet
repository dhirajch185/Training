--------------- Ubuntu basics for trainees -------------

ctrl+shift+c for copying text on ubutnu terminal

ctrl+shift+v for pasting it :)

chmod 777 filename.sh  (To grant execute execute execute permissions for admin, group and user)


sudo poweroff (to shutdown the machine)

sudo reboot (to restart the machine)
----------------- shell commands I normally use -----------------

ls -ltr -> List the files

ps -ef | grep "amazon" (amazon process running on my instance)

top ( cpu and memory metrics)

free (how much ram is still free)

sudo su - ( switch to root user)

./ MyshellScript.sh | more (to execute my shell scripts and read in a better format)

cron    (cron job to schedule the report to check running instances)

scp -i /home/dhiraj/Downloads/aws_login.pem /home/dhiraj/Downloads/aws_login.pem ubuntu@XXAWS PUblic IP AddressXX:/home/ubuntu ( copy files from local machine to aws instance securely)

scp -i /home/dhiraj/Downloads/aws_login.pem /home/dhiraj/scripts/aws_resource_tracker.sh ubuntu@XXAWS PUblic IP AddressXX:/home/ubuntu

aws ec2 describe-instances | jq '.Reservations[].Instances[].InstanceId' (List of instance ids parsed from json response of aws cli.) similary ! yq can be used for parsing data from yaml

--------------------------------- shell script -----------------

!/bin/bash


#########################
#Author: Dhiraj
#Date: XXXXXX 2024

#version: 1

# This scripts records aws instance's resource usage

#########################

set -x
set -e #exit on error - error handling
set -o # exit on error before - pipefail


# AWS S3
# AWS Lambda
# AWS Ec2
# AWS IAM users list

# list s3 buckets on aws
echo "print list of s3 buckets"
aws s3 ls

# list EC2 instances
echo "print list of ec2 buckets"
aws ec2 describe-instances | jq '.Reservations[].Instances[].InstanceId'

# list lambda
echo "print list of lambda functions"
aws lambda list-functions

# list iam users
echo "print list of iam users"
aws iam list-users

----
git init
git add
git commit -m "message for your commit action"


git log filename.sh (gives you the log of commits done for the file)

git reset --hard commitid (commitID is from the git log from above command  -resets the file to thee commit version as per your command)

------------------------------------------------
kubectl get svc -l app.kubernetes.io/created-by=eks-workshop -A

All of our application components are currently using ClusterIP services, which only allows access to other workloads in the same Kubernetes cluster. In order for users to access our application we need to expose the ui application, and in this example we'll do so using a Kubernetes service of type LoadBalancer.


First, take a look at the load balancer itself:

aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-ui-uinlb`) == `true`]'


We can also inspect the targets in the target group that was created by the controller:

ALB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-ui-uinlb`) == `true`].LoadBalancerArn' | jq -r '.[0]')
TARGET_GROUP_ARN=$(aws elbv2 describe-target-groups --load-balancer-arn $ALB_ARN | jq -r '.TargetGroups[0].TargetGroupArn')
aws elbv2 describe-target-health --target-group-arn $TARGET_GROUP_ARN



Get the URL from the Service resource:

kubectl get service -n ui ui-nlb -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}"

To wait until the load balancer has finished provisioning you can run this command:


wait-for-lb $(kubectl get service -n ui ui-nlb -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}")

Let's scale up the ui component to 3 replicas see what happens:

kubectl scale -n ui deployment/ui --replicas=3
kubectl wait --for=condition=Ready pod -n ui -l app.kubernetes.io/name=ui --timeout=60s


If you want to wait to make sure the application still functions the same, run the following command. Otherwise you can proceed to the next module.


wait-for-lb $(kubectl get service -n ui ui-nlb -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}")


First lets install the AWS Load Balancer controller using helm:


helm repo add eks-charts https://aws.github.io/eks-charts
helm upgrade --install aws-load-balancer-controller eks-charts/aws-load-balancer-controller \
  --version "${LBC_CHART_VERSION}" \
  --namespace "kube-system" \
  --set "clusterName=${EKS_CLUSTER_NAME}" \
  --set "serviceAccount.name=aws-load-balancer-controller-sa" \
  --set "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"="$LBC_ROLE_ARN" \
  --wait


---------------------------------------------

Let's inspect the Ingress object created:

kubectl get ingress ui -n ui


aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-ui-ui`) == `true`]'

Inspect the targets in the target group that was created by the controller:


ALB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-ui-ui`) == `true`].LoadBalancerArn' | jq -r '.[0]')
TARGET_GROUP_ARN=$(aws elbv2 describe-target-groups --load-balancer-arn $ALB_ARN | jq -r '.TargetGroups[0].TargetGroupArn')
aws elbv2 describe-target-health --target-group-arn $TARGET_GROUP_ARN


Get the URL from the Ingress resource:


kubectl get ingress -n ui ui -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}"


To wait until the load balancer has finished provisioning you can run this command:


wait-for-lb $(kubectl get ingress -n ui ui -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}")


We can take a look at the ALB listener to see how this works:


ALB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-retailappgroup`) == `true`].LoadBalancerArn' | jq -r '.[0]')
LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn $ALB_ARN | jq -r '.Listeners[0].ListenerArn')
aws elbv2 describe-rules --listener-arn $LISTENER_ARN


Try accessing the new Ingress URL in the browser as before to check the web UI still works:


kubectl get ingress -n ui ui -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}"


Now try accessing the specific path we directed to the catalog service:


ADDRESS=$(kubectl get ingress -n ui ui -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}")
curl $ADDRESS/catalogue | jq .


You'll receive back a JSON payload from the catalog service, demonstrating that we've been able to expose multiple Kubernetes services via the same ALB.

All that we have left to do is install cluster-autoscaler as a helm chart:


helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm upgrade --install cluster-autoscaler autoscaler/cluster-autoscaler \
  --version "${CLUSTER_AUTOSCALER_CHART_VERSION}" \
  --namespace "kube-system" \
  --set "autoDiscovery.clusterName=${EKS_CLUSTER_NAME}" \
  --set "awsRegion=${AWS_REGION}" \
  --set "image.tag=v${CLUSTER_AUTOSCALER_IMAGE_TAG}" \
  --set "rbac.serviceAccount.name=cluster-autoscaler-sa" \
  --set "rbac.serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"="$CLUSTER_AUTOSCALER_ROLE" \
  --wait

Some pods will be in the Pending state, which triggers the cluster-autoscaler to scale out the EC2 fleet.


kubectl get pods -A -o wide --watch

View the cluster-autoscaler logs:


kubectl -n kube-system logs \
  -f deployment/cluster-autoscaler-aws-cluster-autoscaler

View the cluster-autoscaler logs:


kubectl -n kube-system logs \
  -f deployment/cluster-autoscaler-aws-cluster-autoscaler

 You can also follow along with the pod deployment from the command line. You should see the pods transition from pending to running as nodes are scaled up.

Alternatively you can use kubectl:

kubectl get nodes -l workshop-default=yes

In this case we're going to schedule a single pause pod requesting 6.5Gi of memory, which means it will consume almost an entire m5.large instance. This will result in us always having 2 "spare" worker nodes available.

Apply the updates to your cluster:


kubectl apply -k ~/environment/eks-workshop/modules/autoscaling/compute/overprovisioning/setup
kubectl rollout status -n other deployment/pause-pods --timeout 300s

Once this completes the pause pods will be running:


kubectl get pods -n other





