--------------- Ubuntu basics for trainees -------------

ctrl+shift+c for copying text on ubutnu terminal

ctrl+shift+v for pasting it :)

to delete entire line, just press dd at the beginning of the line.

chmod 777 filename.sh  (To grant execute execute execute permissions for admin, group and user)


sudo poweroff (to shutdown the machine)

sudo reboot (to restart the machine)
----------------- shell commands I normally use -----------------

ls -ltr -> List the files

ps -ef | grep "amazon" (amazon process running on my instance)

top ( cpu and memory metrics)

free (how much ram is still free)

sudo su - ( switch to root user)

usermod -aG docker jenkins
usermod -aG docker ubuntu (add users 'jenkins' and 'ubuntu' to docker)

to logout of the user use ---> Ctrl + D



./ MyshellScript.sh | more (to execute my shell scripts and read in a better format)

cron    (cron job to schedule the report to check running instances)

scp -i /home/dhiraj/Downloads/aws_login.pem /home/dhiraj/Downloads/aws_login.pem ubuntu@XXAWS PUblic IP AddressXX:/home/ubuntu ( copy files from local machine to aws instance securely)

scp -i /home/dhiraj/Downloads/aws_login.pem /home/dhiraj/scripts/aws_resource_tracker.sh ubuntu@XXAWS PUblic IP AddressXX:/home/ubuntu

aws ec2 describe-instances | jq '.Reservations[].Instances[].InstanceId' (List of instance ids parsed from json response of aws cli.) similary ! yq can be used for parsing data from yaml

--------------------------------- shell script -----------------

!/bin/bash


#########################
#Author: Dhiraj
#Date: XXXXXX 2024

#version: 1

# This scripts records aws instance's resource usage

#########################

set -x
set -e #exit on error - error handling
set -o # exit on error before - pipefail


# AWS S3
# AWS Lambda
# AWS Ec2
# AWS IAM users list

# list s3 buckets on aws
echo "print list of s3 buckets"
aws s3 ls

# list EC2 instances
echo "print list of ec2 buckets"
aws ec2 describe-instances | jq '.Reservations[].Instances[].InstanceId'

# list lambda
echo "print list of lambda functions"
aws lambda list-functions

# list iam users
echo "print list of iam users"
aws iam list-users

----
git init
git add
git commit -m "message for your commit action"


git log filename.sh (gives you the log of commits done for the file)

git reset --hard commitid (commitID is from the git log from above command  -resets the file to thee commit version as per your command)

------------------------------------------------
kubectl get svc -l app.kubernetes.io/created-by=eks-workshop -A

All of our application components are currently using ClusterIP services, which only allows access to other workloads in the same Kubernetes cluster. In order for users to access our application we need to expose the ui application, and in this example we'll do so using a Kubernetes service of type LoadBalancer.


First, take a look at the load balancer itself:

aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-ui-uinlb`) == `true`]'


We can also inspect the targets in the target group that was created by the controller:

ALB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-ui-uinlb`) == `true`].LoadBalancerArn' | jq -r '.[0]')
TARGET_GROUP_ARN=$(aws elbv2 describe-target-groups --load-balancer-arn $ALB_ARN | jq -r '.TargetGroups[0].TargetGroupArn')
aws elbv2 describe-target-health --target-group-arn $TARGET_GROUP_ARN



Get the URL from the Service resource:

kubectl get service -n ui ui-nlb -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}"

To wait until the load balancer has finished provisioning you can run this command:


wait-for-lb $(kubectl get service -n ui ui-nlb -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}")

Let's scale up the ui component to 3 replicas see what happens:

kubectl scale -n ui deployment/ui --replicas=3
kubectl wait --for=condition=Ready pod -n ui -l app.kubernetes.io/name=ui --timeout=60s


If you want to wait to make sure the application still functions the same, run the following command. Otherwise you can proceed to the next module.


wait-for-lb $(kubectl get service -n ui ui-nlb -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}")


First lets install the AWS Load Balancer controller using helm:


helm repo add eks-charts https://aws.github.io/eks-charts
helm upgrade --install aws-load-balancer-controller eks-charts/aws-load-balancer-controller \
  --version "${LBC_CHART_VERSION}" \
  --namespace "kube-system" \
  --set "clusterName=${EKS_CLUSTER_NAME}" \
  --set "serviceAccount.name=aws-load-balancer-controller-sa" \
  --set "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"="$LBC_ROLE_ARN" \
  --wait


---------------------------------------------

Let's inspect the Ingress object created:

kubectl get ingress ui -n ui


aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-ui-ui`) == `true`]'

Inspect the targets in the target group that was created by the controller:


ALB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-ui-ui`) == `true`].LoadBalancerArn' | jq -r '.[0]')
TARGET_GROUP_ARN=$(aws elbv2 describe-target-groups --load-balancer-arn $ALB_ARN | jq -r '.TargetGroups[0].TargetGroupArn')
aws elbv2 describe-target-health --target-group-arn $TARGET_GROUP_ARN


Get the URL from the Ingress resource:


kubectl get ingress -n ui ui -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}"


To wait until the load balancer has finished provisioning you can run this command:


wait-for-lb $(kubectl get ingress -n ui ui -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}")


We can take a look at the ALB listener to see how this works:


ALB_ARN=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-retailappgroup`) == `true`].LoadBalancerArn' | jq -r '.[0]')
LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn $ALB_ARN | jq -r '.Listeners[0].ListenerArn')
aws elbv2 describe-rules --listener-arn $LISTENER_ARN


Try accessing the new Ingress URL in the browser as before to check the web UI still works:


kubectl get ingress -n ui ui -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}"


Now try accessing the specific path we directed to the catalog service:


ADDRESS=$(kubectl get ingress -n ui ui -o jsonpath="{.status.loadBalancer.ingress[*].hostname}{'\n'}")
curl $ADDRESS/catalogue | jq .


You'll receive back a JSON payload from the catalog service, demonstrating that we've been able to expose multiple Kubernetes services via the same ALB.

All that we have left to do is install cluster-autoscaler as a helm chart:


helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm upgrade --install cluster-autoscaler autoscaler/cluster-autoscaler \
  --version "${CLUSTER_AUTOSCALER_CHART_VERSION}" \
  --namespace "kube-system" \
  --set "autoDiscovery.clusterName=${EKS_CLUSTER_NAME}" \
  --set "awsRegion=${AWS_REGION}" \
  --set "image.tag=v${CLUSTER_AUTOSCALER_IMAGE_TAG}" \
  --set "rbac.serviceAccount.name=cluster-autoscaler-sa" \
  --set "rbac.serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"="$CLUSTER_AUTOSCALER_ROLE" \
  --wait

Some pods will be in the Pending state, which triggers the cluster-autoscaler to scale out the EC2 fleet.


kubectl get pods -A -o wide --watch

View the cluster-autoscaler logs:


kubectl -n kube-system logs \
  -f deployment/cluster-autoscaler-aws-cluster-autoscaler

View the cluster-autoscaler logs:


kubectl -n kube-system logs \
  -f deployment/cluster-autoscaler-aws-cluster-autoscaler

 You can also follow along with the pod deployment from the command line. You should see the pods transition from pending to running as nodes are scaled up.

Alternatively you can use kubectl:

kubectl get nodes -l workshop-default=yes

In this case we're going to schedule a single pause pod requesting 6.5Gi of memory, which means it will consume almost an entire m5.large instance. This will result in us always having 2 "spare" worker nodes available.

Apply the updates to your cluster:


kubectl apply -k ~/environment/eks-workshop/modules/autoscaling/compute/overprovisioning/setup
kubectl rollout status -n other deployment/pause-pods --timeout 300s

Once this completes the pause pods will be running:


kubectl get pods -n other


--------------------
prepare-environment autoscaling/compute/karpenter


Karpenter's goal is to improve the efficiency and cost of running workloads on Kubernetes clusters. Karpenter works by:

Watching for pods that the Kubernetes scheduler has marked as unschedulable
Evaluating scheduling constraints (resource requests, node selectors, affinities, tolerations, and topology spread constraints) requested by the pods
Provisioning nodes that meet the requirements of the pods
Scheduling the pods to run on the new nodes
Removing the nodes when the nodes are no longer needed


The first thing we'll do is install Karpenter in our cluster. Various pre-requisites were created during the lab preparation stage, including:

An IAM role for Karpenter to call AWS APIs
An IAM role and instance profile for the EC2 instances that Karpenter creates
An EKS cluster access entry for the node IAM role so the nodes can join the EKS cluster
An SQS queue for Karpenter to receive Spot interruption, instance re-balance and other events


helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
  --version "${KARPENTER_VERSION}" \
  --namespace "karpenter" --create-namespace \
  --set "settings.clusterName=${EKS_CLUSTER_NAME}" \
  --set "settings.interruptionQueue=${KARPENTER_SQS_QUEUE}" \
  --set controller.resources.requests.cpu=1 \
  --set controller.resources.requests.memory=1Gi \
  --set controller.resources.limits.cpu=1 \
  --set controller.resources.limits.memory=1Gi \
  --set replicas=1 \
  --wait

Karpenter will be running as a deployment in the karpenter namespace:

kubectl get deployment -n karpenter

Karpenter configuration comes in the form of a NodePool CRD (Custom Resource Definition). A single Karpenter NodePool is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. A cluster may have more than one NodePool, but for the moment we'll declare a default one.

One of the main objectives of Karpenter is to simplify the management of capacity. If you're familiar with other auto scaling solutions, you may have noticed that Karpenter takes a different approach, referred to as group-less auto scaling. Other solutions have traditionally used the concept of a node group as the element of control that defines the characteristics of the capacity provided (i.e: On-Demand, EC2 Spot, GPU Nodes, etc) and that controls the desired scale of the group in the cluster. In AWS the implementation of a node group matches with Auto Scaling groups. Karpenter allows us to avoid complexity that arises from managing multiple types of applications with different compute needs.

We'll start by applying some custom resources used by Karpenter. First we'll create a NodePool that defines our general capacity requirements:

~/environment/eks-workshop/modules/autoscaling/compute/karpenter/nodepool/nodepool.yaml

We're asking the NodePool to start all new nodes with a Kubernetes label type: karpenter, which will allow us to specifically target Karpenter nodes with pods for demonstration purposes

B
The NodePool CRD supports defining node properties like instance type and zone. In this example, we're setting the karpenter.sh/capacity-type to initially limit Karpenter to provisioning On-Demand instances, as well as node.kubernetes.io/instance-type to limit to a subset of specific instance types. You can learn which other properties are available here. We'll work on a few more during the workshop.

C
A NodePool can define a limit on the amount of CPU and memory managed by it. Once this limit is reached Karpenter will not provision additional capacity associated with that particular NodePool, providing a cap on the total compute.

And we'll also need an EC2NodeClass which provides the specific configuration that applies to AWS:

~/environment/eks-workshop/modules/autoscaling/compute/karpenter/nodepool/nodeclass.yaml

The subnetSelectorTerms can be used to look up the subnets where Karpenter should launch the EC2 instances. These tags were automatically set on the associated AWS infrastructure provided for the workshop. securityGroupSelectorTerms accomplishes the same function for the security group that will be attached to the EC2 instances.

B
We define a set of tags that will be applied to EC2 instances created which enables accounting and governance.

We've now provided Karpenter with the basic requirements in needs to start provisioning capacity for our cluster.

Apply the NodePool and EC2NodeClass with the following command:


kubectl kustomize ~/environment/eks-workshop/modules/autoscaling/compute/karpenter/nodepool \
  | envsubst | kubectl apply -f-


Throughout the workshop you can inspect the Karpenter logs with the following command to understand its behavior:


kubectl logs -l app.kubernetes.io/instance=karpenter -n karpenter | jq '.'

You'll notice in this example we're using the image:

public.ecr.aws/eks-distro/kubernetes/pause

This is a small container that will consume no real resources and starts quickly, which makes it great for demonstrating scaling scenarios. We'll be using this for many of the examples in this particular lab.


kubectl apply -k ~/environment/eks-workshop/modules/autoscaling/compute/karpenter/scale


Now let's deliberately scale this deployment to demonstrate that Karpenter is making optimized decisions. Since we've requested 1Gi of memory, if we scale the deployment to 5 replicas that will request a total of 5Gi of memory.

Before we proceed, what instance from the table above do you think Karpenter will end up provisioning? Which instance type would you want it to?


kubectl scale -n other deployment/inflate --replicas 5


Because this operation is creating one or more new EC2 instances it will take a while, you can use kubectl to wait until its done with this command:

kubectl rollout status -n other deployment/inflate --timeout=180s


Once all of the Pods are running, lets see what instance type it selecting:


kubectl logs -l app.kubernetes.io/instance=karpenter -n karpenter | grep 'launched nodeclaim' | jq '.'

The pods that we scheduled will fit nicely in to an EC2 instance with 8GB of memory, and since Karpenter will always prioritize the lowest price instance type for on-demand instances, it will select m5.large.

info
There are certain cases where a different instance type might be selected other than the lowest price, for example if that cheapest instance type has no remaining capacity available in the region you're working in

We can also check the metadata added to the node by Karpenter:


kubectl get node -l type=karpenter -o jsonpath='{.items[0].metadata.labels}' | jq '.'


This simple examples illustrates the fact that Karpenter can dynamically select the right instance type based on the resource requirements of the workloads that require compute capacity. This differs fundamentally from a model oriented around node pools, such as Cluster Autoscaler, where the instance types within a single node group must have consistent CPU and memory characteristics.


Karpenter automatically discovers nodes that are eligible for disruption and spins up replacements when needed. This can happen for three different reasons:

Expiration: By default, Karpenter automatically expires instances after 720h (30 days), forcing a recycle allowing nodes to be kept up to date.
Drift: Karpenter detects changes in configuration (such as the NodePool or EC2NodeClass) to apply necessary changes
Consolidation: A critical feature for operating compute in a cost-effective manner, Karpenter will optimize our cluster's compute on an on-going basis. For example, if workloads are running on under-utilized compute instances, it will consolidate them to fewer instances.
Disruption is configured through the disruption block in a NodePool. You can see highlighted below the policy thats already configured in our NodePool.

~/environment/eks-workshop/modules/autoscaling/compute/karpenter/nodepool/nodepool.yaml

The consolidationPolicy can also be set to WhenEmpty, which restricts disruption only to nodes that contain no workload pods. Learn more about Disruption on the Karpenter docs.

Scaling out infrastructure is only one side of the equation for operating compute infrastructure in a cost-effective manner. We also need to be able to optimize on an on-going basis such that, for example, workloads running on under-utilized compute instances are compacted to fewer instances. This improves the overall efficiency of how we run workloads on the compute, resulting in less overhead and lower costs.

Let's explore how to trigger automatic consolidation when disruption is set to consolidationPolicy: WhenUnderutilized:

Scale the inflate workload from 5 to 12 replicas, triggering Karpenter to provision additional capacity
Scale down the workload back down to 5 replicas
Observe Karpenter consolidating the compute
Scale our inflate workload again to consume more resources:


kubectl scale -n other deployment/inflate --replicas 12
kubectl rollout status -n other deployment/inflate --timeout=180s

This changes the total memory request for this deployment to around 12Gi, which when adjusted to account for the roughly 600Mi reserved for the kubelet on each node means that this will fit on 2 instances of type m5.large:


kubectl get nodes -l type=karpenter --label-columns node.kubernetes.io/instance-type

Next, scale the number of replicas back down to 5:


kubectl scale -n other deployment/inflate --replicas 5

We can check the Karpenter logs to get an idea of what actions it took in response to our scaling in the deployment. Wait about 5-10 seconds before running the following command:


kubectl logs -l app.kubernetes.io/instance=karpenter -n karpenter | grep 'consolidation delete' | jq '.'


This will result in the Kubernetes scheduler placing any pods on those nodes on the remaining capacity, and now we can see that Karpenter is managing a total of 1 node:


kubectl get nodes -l type=karpenter


Karpenter can also further consolidate if a node can be replaced with a cheaper variant in response to workload changes. This can be demonstrated by scaling the inflate deployment replicas down to 1, with a total memory request of around 1Gi:


kubectl scale -n other deployment/inflate --replicas 1



kubectl logs -l app.kubernetes.io/instance=karpenter -n karpenter -f | jq '.'

The previous command includes the flag "-f" for follow, allowing us to watch the logs as they happen. Consolidation to a smaller node takes less than one minute. Watch the logs to how the Karpenter controller behaves.


kubectl get nodes -l type=karpenter -o jsonpath="{range .items[*]}{.metadata.labels.node\.kubernetes\.io/instance-type}{'\n'}{end}"



Uninstall StormForge Agent and Applier
To uninstall the StormForge Applier, run the following command:

helm uninstall stormforge-applier -n stormforge-system
To uninstall the StormForge Agent, run the following command:

helm uninstall stormforge-agent -n stormforge-system
Uninstall the StormForge Sample App
To uninstall the StormForge Sample App, run the following command:

helm uninstall optlive-showcase-app -n default


--------------------------------------------------------------------------------------------

ssh-keygen (this commmand in a linux terminal will create the files --> id_rsa  id_rsa.pub  known_hosts  known_hosts.old)
-> in these files id_rsa.pub is the public key which can be shared to make passwordless authentication to the target server at anytime. if the public key is added to the target server's authorized_keys file.

authorized_keys file should be in .ssh folder. Navigate using the command (ls ~/.ssh/)

---------------------------------------------------

Ansible examples

ansible uses ssh to connect to the host and push the commands using the modules. Remember you cant 


Modules
https://docs.ansible.com/ansible/latest/collections/ansible/builtin/find_module.html#ansible-collections-ansible-builtin-find-module

using the file module in 'ansible adhoc' mode

example: dhiraj@dhiraj-VirtualMachine:~$  ansible localhost -m find -a "path=Downloads file_type=file"

-----------

first.yaml file contents

---
- name: My First Playbook
  hosts: localhost
  become: true
  become_user: root

  vars:
    ansible_python_interpreter: /usr/bin/python3

  tasks:
  - name: Test reachability task
    ping:

  - name: Find files
    ansible.builtin.find:
      paths: /home/dhiraj/Downloads
      recurse: yes
      file_type: file

Executed the yaml file successfully using ansible-playbook using --->> dhiraj@dhiraj-VirtualMachine:~/Downloads$ sudo ansible-playbook first.yaml -v

