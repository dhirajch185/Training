K8s Setup Process with kubeadm

1) Provision Systems or Virtual Machines
2) Designate Cluster Roles - Assign the role of the master to one node while designating the remaining nodes as workers.
3) Install a Container Runtime - Install a container runtime on each node. like (ContainerD)
4) Install kubeadm - Deploy the kubeadm tool on every node. kubeadm is critical as it bootstraps the cluster by installing and configuring the necessary components in the correct sequence.
5) Initialize the Master Node - Use kubeadm to initialize the master node. This step sets up vital components and configures the master server to manage the cluster effectively.
6) Configure the Pod Network - Before integrating worker nodes into the cluster, verify that the pod network is properly configured. A specialized networking solution is required for reliable communication between the master and worker nodes.
7) Join Worker Nodes - After the pod network is set up, add worker nodes to the cluster by having them join the master node.
8) Deploy Applications - Once all nodes are successfully configured and connected, deploy applications within the Kubernetes cluster.
---------------------------------
Install the kubeadm and kubelet packages on the controlplane and node01 nodes.
Use the exact version of 1.32.0-1.1 for both.
----------------
check kubelet version --- kubelet --version
----------

5 / 8
Initialize Control Plane Node (Master Node). Use the following options:


apiserver-advertise-address - Use the IP address allocated to eth0 on the controlplane node

apiserver-cert-extra-sans - Set it to controlplane

pod-network-cidr - Set to 172.17.0.0/16

service-cidr - Set to 172.20.0.0/16

Once done, set up the default kubeconfig file and wait for node to be part of the cluster.

-------->

You can use the below kubeadm init command to spin up the cluster:


IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=172.17.0.0/16 --service-cidr=172.20.0.0/16
Once you run the init command, you should see an output similar to below:


[init] Using Kubernetes version: v1.32.1
[preflight] Running pre-flight checks
        [WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
W0128 11:41:34.487538   10032 checks.go:846] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.10" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [controlplane kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [172.20.0.1 192.168.114.75]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [controlplane localhost] and IPs [192.168.114.75 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [controlplane localhost] and IPs [192.168.114.75 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.824641ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 20.501525032s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node controlplane as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node controlplane as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: aqe7sk.zyr2zpqxa0aomjje
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.114.75:6443 --token aqe7sk.zyr2zpqxa0aomjje \
        --discovery-token-ca-cert-hash sha256:b9fa5652e076b79889b7ceb2f41638be5d6975c2c2970b27ed6c9e601315cd41 


Once the command has been run successfully, set up the kubeconfig:

mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

-------->
These steps have to be performed on both nodes.

set net.bridge.bridge-nf-call-iptables to 1:

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system
The container runtime has already been installed on both nodes, so you may skip this step.
Install kubeadm, kubectl and kubelet on all nodes:

sudo apt-get update

sudo apt-get install -y apt-transport-https ca-certificates curl

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update

# To see the new version labels
sudo apt-cache madison kubeadm

sudo apt-get install -y kubelet=1.32.0-1.1 kubeadm=1.32.0-1.1 kubectl=1.32.0-1.1

sudo apt-mark hold kubelet kubeadm kubectl

-------------------------------------------

kubectl get pods
kubectl get pods -o wide
kubectl describe pod <podname>
kubectl create -f <podcreation.yaml filename> ---------- Delcarative Way
kubectl run mynginx_podname --image=nginx_dockerHubImageName ---- imperative way
kubectl describe pod <pod_name> | grep -c "Containers:"  ---------> to find the number of containers used in the pod
kubectl create -f .\rc-definition.yaml (Create replication Controller)
kubectl delete rc myapp-rc ----- Delete replication controller
kubectl get replicaset
kubectl replace -f replicaset-definition.yaml ( will replace the existing replicaset with any new changes) --- Declarative way
kubectl replace --force -f pod-definition.yaml ( will delete and recreate the pod)

************ your linux version?? ---->  cat /etc/*release*


kubectl scale --replicas=6 -f replicaset-definition.yaml -- Imperative way
kubectl scale --replicas=6 replicaset myapp-replicaset --- Imperative way with type & name format
kubectl edit replicaset myapp-replicaset ------------ Risky.. Imperative way to make changes to the replicaset. Note: Careful.. Saving the edited file will immediately apply the changes.
kubectl delete replicaset myapp-replicaset -- Delete replicaset
kubectl create -f deployment-definition.yaml
kubectl create deployment --image=nginx nginx
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml  ---------> Generate Deployment YAML file (-o yaml). Don’t create it(–dry-run)

kubectl create deployment --replicas=3 --image=httpd:2.4-alpine httpd-frontend --dry-run=client -o yaml > httpd-deployment-definition.yaml  ----> created a file as required

kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml  -----> deployment yaml file with replicas as needed

kubectl get deployments
kubectl get all  ----------------- Shows all the resources created
kubectl get pods,svc ----------------- Shows just pods and services created
kubectl get pods --watch ----------- will show the status of the pods continuously if it changes.
kubectl rollout status deployment/myapp-deployment
kubectl apply -f deployment-definition.yaml  ----------- Make changes to the deployment yaml (like image version) and upgrade
kubectl set image deployment/myapp-deployment \ nginx-container=nginx:1.9.1 ------- upgrading imperatively using container name (only for toubelshooting)

kubectl rollout status deployment.apps/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment

******** Services nodePort range is -- 30K to 32767 ******
-- service uses random algorithm to distribute load across all the pods which it had identified based on the service definition file labels which matched with pods labels from their definition.
kubectl create -f .\service-definition.yaml
kubectl get services
minikube service myapp-service --url -------------------- to get the url of service running on minikube node

kubectl delete pods --all -n default ------- Deltes all the pods running on the namespace by the name 'default'


---------------------namespace ---------------------

kubectl config set-context $(kubectl config current-context) --namespace=dev  --> Permanently switch to a namespace so no namespace mentioned everytime
kubectl get pods --all-namespaces -----> get pods form all namespaces
------------- Troubleshooting -----------------
After fixing the image name, you can re-deploy the changes using below command
kubectl apply -f podCreate.yaml
         or
kubectl edit pod <podName>
Then check the pod details, you should see the STATUS as running

---------------- Terminologies ---------------
replication controller - create multiple pods or nodes to load balance. (older technology and obsolete)
Replica set - new way of implementation (apiversion : apps/v1, selector mandatory)
Poda - apiVersion is v1
Rolling upgrade.
cillium/flannel - used for internal routing, ip address allocation between nodes and pods in a cluster
NodePort (range 30000 to 32767)
ClusterIP
LoadBalancer


--------------------- Binding for Manual scheduling ---------------
create a 'Binding' object (yaml file) to assign a node name to a existing pod and send a post request to the pod's binding api.
Thus mimicking what the actual scheduler does.

If a scheduler is missing, to assign a node manually to a pod, use the field 'nodeName: node01' to the pod definition to get the job done.

---------------- scenarios -------------
kubectl get pods --selector app=App1     -> filters all the pods with the label selector app=App1
 k get pods --selector bu=finance | wc -l  ---> counts the pods along with header
k get pod --selector env=prod,bu=finance,tier=frontend  --> multiple filters

kubectl get all --selector env=prod --no-headers | wc -l  ------> count without headers

kubectl expose pod redis --port=6379 --name redis-service --------> Create a service redis-service to expose the redis application within the cluster on port 6379.

kubectl run httpd --image=httpd:alpine --port=80 --expose ------> Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80. Try to do this with as few steps as possible.


kubectl taint nodes node-name key=value:taint-effect ----> to taint (label) the nodes so pods which do not tolerate (match the labels) the taint will not be placed in the node with the taint value.

Pods which do not tolerate the taint will have 3 response status ---> NoSchedule (pods will not be scheduled on the nodes), PreferNoSchedule (system will try to place the pod on the node but 'no gurantee'), NoExecute (New pods will not be scheduled and any existing pods which do not tolerate the taint will be evicted as well from the node) 

example: kubectl taint nodes node1 app=blue:NoSchedule

Tolerations are added to the pods with tolerations added to spec of the pod yaml defition.
spec:
  tolerations:
    - key: "app"
      operator: "Equal"
      value: "blue"
      effect: "NoSchedule"
*********Note: taint and toleration doesnt mean the tolerated pod always ends up in a tainted node. it can be distributed to untainted node as well. This is just to restrict the specific nodes to have only certain type of pods. To ensure required pods to end up to required nodes, we use a different concept called 'NodeSelector' and 'NodeAffinity' 

kubectl label nodes <node name> <label-key>= <lable-value>  ------> To label a node with 'nodeselctor' property under spec in Pod definition yaml file.
eg: kubecl label nodes node01 size=Large

----------> Run the command: kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule- to untaint the node. ************ Observe the NoSchedule have a '-' at the end to untaint it????

------- Pod lifecycle -- stages
Scheduler places a pod in a node based on these affinity rules
requiredDuringSchedulingIgnoreDuringExecution
preferredDuringScheduingIgnoreDuringExecution

During Scheduing - Pod does not exist and created for the first time
During Execution - This is where the changes is made in the label of the node after the pods are allocated to the node.

Affinity rules are considered and pods are placed on the right nodes when the pods are newly created.
If nodes with matching labels are not available, with requiredDuringSchedulingIgnoreDuringExecution type, the scheduler will mandate that the pod will be placed on a node with given affinity rules. If it cannot find one, the pod will not be scheduled. If placement of pod is crucial, this type will be used. If its not crucial then use preferredDuringScheduingIgnoreDuringExecution, that way it will place it if it cant find the mathing labels.

IgnoreDuringExecution means, already allocated pods doesnt have any change if node lable is made.


preferredDuringScheduingRequiredDuringExecution -- This is when after label change, the pods are evicted from the node if the nodeAffinity rules doesn't match.
--------------------------------------------
kubectl run redis --image=redis --dry-run=client -o yaml > redis-pod.yaml -------->to create a yaml file from 'kubectl run' command is like. Reember --dry-run will not create the resource just the yaml file.

kubectl get replicaset new-replica-set -o yaml > replicaset-definition.yaml

----------------------------------- Expose Vs Service --------------------------------------------

Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
(This will automatically use the pod’s labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
(This will not use the pods labels as selectors, instead, it will assume selectors as app=redis.

You cannot pass in selectors as an option.

So, it does not work very well if your pod has a different label set. So, generate the file and modify the selectors before creating the service)

Create a Service named nginx of type NodePort to expose pod nginx’s port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
(This will automatically use the pod’s labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
(This will not use the pod labels as selectors.)

Both the above commands have their own challenges. While one of them cannot accept a selector, the other cannot accept a node port. I would recommend going with the

kubectl expose
command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.


************ kubectl apply (Declarative way) *****************
Kubernetes nomally takes local config file (stored on our local system),  a live object definition (stored in kubernetes memory)..  on kubernetes and 'last applied configuration'(stored in Live ojbect configuration as an 'annotation' ) before deciding what changes to be made.
If the object doesn't exist, an object is created.
Ideally, an object similar to what we had defined locally is created within kubernetes, but also with additional fields to store status of the object. This is the Live object configuration of the object in kubernetes cluster.
when we do kubectl apply, it does a bit more... the yaml of the local is converted to a json format and then stored as 'last applied configuration' 
For any upgrades, all three objects are compared to understand what changes are to be made to the live object.
If any field is removed from local configuration, the 'last applied config' helps to identify the removed fields and 'live object configuration' is upgraded accordingly.
Note This is only done with kubectl apply(declarative) not 'kubectl create'(imperative) or 'kubectl replace'. They don't store 'last applied configuration' like this.

*********************************************** Static Pod Path ****************
static pods will have nodename appended to them.
another way is to look at its yaml definition and check its 'OwnerReference'. if its a 'replicaSet' then its not a staic pod. if ownerReference is a 'node' its a static pod.


What is the path of the directory holding the static pod definition files? --> Run the command ps -aux | grep kubelet and identify the config file - --config=/var/lib/kubelet/config.yaml. Then check in the config file for staticPodPath. --> found this --config=/var/lib/kubelet/config.yaml --> now do this --> cat /var/lib/kubelet/config.yaml | grep 'staticPodPath'
output is --> staticPodPath: /etc/kubernetes/manifests


ps -ef | grep kube-apiserver | grep admission-plugins -------> Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.

Another way is
kubectl get pods -n kube-system.. get details of kubeapiserver. Then grep using the command
kubectl exec -it kube-apiserver-controlplane -n kube-system --kube-apiserver -h | grep 'enable-admission-plugins'


**************************** webhooks ****************

Create TLS secret webhook-server-tls for secure webhook communication in webhook-demo namespace.

We have already created below cert and key for webhook server which should be used to create secret.

Certificate : /root/keys/webhook-server-tls.crt

Key : /root/keys/webhook-server-tls.key


--> kubectl -n webhook-demo create secret tls webhook-server-tls \
    --cert "/root/keys/webhook-server-tls.crt" \
    --key "/root/keys/webhook-server-tls.key"

************************** Check secuirty context of the pod ******************

controlplane ~ ➜  k get pod pod-with-defaults -o yaml | grep -A2 "securityContext"
  securityContext:
    runAsNonRoot: true
    runAsUser: 1234

*************************************************************************

# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]


---------------->
Deploy a pod with a conflicting securityContext i.e. pod running with a user id of 0 (root)


We have added pod definition file under /root/pod-with-conflict.yaml

Mutating webhook should reject the request as its asking to run as root user without setting runAsNonRoot: false
------ output  -----> controlplane ~ ➜  k create -f pod-with-conflict.yaml 
Error from server: error when creating "pod-with-conflict.yaml": admission webhook "webhook-server.webhook-demo.svc" denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)


****************************** install metrics server with yaml manifest*********************

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

k top nodes -----------> for nodes resource consumption
k top pods ------->for pods resource consumption

k top pods -n default --sort-by='memory'
k top pods -n default --sort-by='cpu'| tail -1 -------> least cpu consuming pod

*************************** inspect logs **************************

k logs webapp-2

kubectl logs <pod_name> -c <container_name>     ---> To inspect logs of a specific container in a pod

-------------------------------- command & Args -------------------------
Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. 

---
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"


*************************** secrets ************
kubectl create secret generic Secret_name --from-literal=<secret_key1>=<secret_value1> \
                                          --from-literal=<secret_key2>=<secret_value2> ------> Imperative way

kubectl create secret generic Secret_name --from-file=<filepath>
eg: kubectl create secret generic app_secret --from-file=app_secret.properties  -----> imperative way

-----------------------
apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
ata:
  ssh-privatekey: UG91cmluZzYlRW1vdGljb24lU2N1YmE=    

kubectl create -f secretSshauth.yaml -------> declarative way

to encrypt, we can use this command in terminal ...> echo -n "Pouring6%Emoticon%Scuba" | base64

echo -n "Pouring6%Emoticon%Scuba" | base64 --decode

to view the secret --> kubectl get secret secret-ssh-auth -o yaml
----------------------------------------------


apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ["sleep2.0"]
      args: ["10"]

-----------------------------------------------------  HPA autoscaler -----------------------------------------------------

YAML file for autoscaler--->
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 80
status:
  currentReplicas: 0
  desiredReplicas: 0

To run the yaml file to autoscale --> kubectl apply -f /root/autoscale.yml ------> Declarative
kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80 --> imperative

------------------------------------------------------------  VPA autoscaler ---------------------------------------------
We have prepared the required YAML files for you to deploy the Vertical Pod Autoscaler (VPA). Simply follow the steps below to apply the necessary configurations:

Step 1: Install VPA Custom Resource Definitions (CRDs)  <<-----------------------------
These CRDs allow Kubernetes to recognize the custom resources that VPA uses to function properly. To install them, run this command:

kubectl apply -f /root/vpa-crds.yml

Step 2: Install VPA Role-Based Access Control (RBAC)   <<---------------------------
RBAC ensures that VPA has the appropriate permissions to operate within your Kubernetes cluster. To install the RBAC settings, run:

kubectl apply -f /root/vpa-rbac.yml

By running these commands, the VPA will be successfully deployed to your cluster, ready to manage and adjust your pod resources dynamically.
------------------------------------------

Clone the VPA Repository and Set Up the Vertical Pod Autoscaler
You are required to clone the Kubernetes Autoscaler repository into the /root directory and set up the Vertical Pod Autoscaler (VPA) by running the provided script.

Steps:
Clone the repository:

First, navigate to the /root directory and clone the repository:

  git clone https://github.com/kubernetes/autoscaler.git

Navigate to the Vertical Pod Autoscaler directory:

After cloning, move into the vertical-pod-autoscaler directory:

   cd autoscaler/vertical-pod-autoscaler

Run the setup script:

Execute the provided script to deploy the Vertical Pod Autoscaler:

   ./hack/vpa-up.sh

By following these steps, the Vertical Pod Autoscaler will be installed and ready to manage pod resources in your Kubernetes cluster.


---------------------------------------------------------------------------
Which of the following are the VPA CRDs that get installed as part of the Vertical Pod Autoscaler setup? --> kubectl get crds | grep verticalpodautoscaler

check Custom Resource Definitions (CRD) --> kubectl get crd

--------------------
VPA deployments in kube-system namespace

k get deploy -n kube-system | grep vpa-*
vpa-admission-controller   1/1     1            1           3m53s
vpa-recommender            1/1     1            1           3m54s
vpa-updater                1/1     1            1           3m54s

check logs for vpa updater -------> k -n kube-system logs vpa-updater-pod_name 
-----------------------------------------------------------------------------------------------

Drain the nodes from a specific node so the pods can be recreated on another node. The node will be cordoned(marked unschedulable). to make them schedulable again, we need to uncordon.

kubectl drain node01

kubectl uncordon node01

kubectl cordon node02 ---> will just mark the node unschedulable but doesn't drain the pods off the node.

-------------------------------------------------------------------------- upgrade clusters --------------------------

To check latest version of components and plan for upgrade using kubeadm tool ---------> kubeadm upgrade plan

Always upgrade controlplane first. Drain all the pods from the controlplane before updating it.

To upgrade Kubernetes version

To seamlessly transition from Kubernetes v1.31 to v1.32 and gain access to the packages specific to the desired Kubernetes minor version, follow these essential steps during the upgrade process. This ensures that your environment is appropriately configured and aligned with the features and improvements introduced in Kubernetes v1.32.

On the controlplane node:

Use any text editor you prefer to open the file that defines the Kubernetes apt repository.

vim /etc/apt/sources.list.d/kubernetes.list
Update the version in the URL to the next available minor release, i.e v1.32.

deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
After making changes, save the file and exit from your text editor. Proceed with the next instruction.

apt update

apt-cache madison kubeadm
Based on the version information displayed by apt-cache madison, it indicates that for Kubernetes version 1.32.0, the available package version is 1.32.0-1.1. Therefore, to install kubeadm for Kubernetes v1.32.0, use the following command:

apt-get install kubeadm=1.32.0-1.1
Run the following command to upgrade the Kubernetes cluster.

kubeadm upgrade plan v1.32.0

kubeadm upgrade apply v1.32.0
Note that the above steps can take a few minutes to complete.

Now, upgrade the Kubelet version. Also, mark the node (in this case, the "controlplane" node) as schedulable.

apt-get install kubelet=1.32.0-1.1
Run the following commands to refresh the systemd configuration and apply changes to the Kubelet service:

systemctl daemon-reload

systemctl restart kubelet

----------------------------------------------------------------------------------------------------------------------

If you are on the controlplane node, run ----------->> ssh node01 to log in to the node01.

-----------------------------------------------------------------------------------------------------------



https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

------------------------------------------------------

WORKING WITH ETCDCTL

etcdctl is a command line client for etcd.

In all our Kubernetes Hands-on labs, the ETCD key-value database is deployed as a static pod on the master. The version used is v3.

To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3.

You can do this by exporting the variable ETCDCTL_API prior to using the etcdctl client. This can be done as follows:

export ETCDCTL_API=3

On the Master Node:

Image

To see all the options for a specific sub-command, make use of the -h or –help flag.

For example, if you want to take a snapshot of etcd, use:

etcdctl snapshot save -h and keep a note of the mandatory global options.

Since our ETCD database is TLS-Enabled, the following options are mandatory:

–cacert               verify certificates of TLS-enabled secure servers using this CA bundle

–cert                  identify secure client using this TLS certificate file

–endpoints=[127.0.0.1:2379] This is the default as ETCD is running on master node and exposed on localhost 2379.

–key                 identify secure client using this TLS key file

For a detailed explanation on how to make use of the etcdctl command line tool and work with the -h flags.
----------------------------------------------------------------------------------------------------------
At what address can you reach the ETCD cluster from the controlplane node?

Check the ETCD Service configuration in the ETCD POD

Use the command kubectl describe pod etcd-controlplane -n kube-system and look for --listen-client-urls

----------------------------use case for etcd backup ----------
The master node in our cluster is planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.


Store the backup file at location /opt/snapshot-pre-boot.db

solution-> use the command to grep for path ------> k -n kube-system describe pod etcd-controlplane | grep '\-\-' (Remember -> Since our ETCD database is TLS-Enabled, the following options are mandatory)


root@controlplane:~# ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db

Snapshot saved at /opt/snapshot-pre-boot.db
root@controlplane:~# 

----------------------------------------------

To restore the stored snapshot -------->

root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db



Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.



Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup).

  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want).

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.



Note 1: As the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run the command: watch "crictl ps | grep etcd" to see when the ETCD pod is restarted.

Note 2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.

Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.



If you do change --data-dir to /var/lib/etcd-from-backup in the ETCD YAML file, make sure that the volumeMounts for etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

-------------------------------------------------------------------------------------------------------------------------------------------

how many clusters are part of the node?------------------> k config get-clusters
switch to specific cluster context to get details of that cluster --> k config use-context cluster1
list the clusters and their contexts --> k config view

To get the default data directory used the for ETCD datastore used in cluster, connect to etcd using ssh and run --> ps -ef | grep --color=always 'etcd.*--data-dir'
etcd's ip address or hostname can be found in api server's details ---> k -n kube-system describe pod kube-apiserver-clusterxx-controlplane at--> --etcd-servers= parameter

-------------------------------------
If you check out the pods running in the kube-system namespace in cluster2, you will notice that there are NO etcd pods running in this cluster!

student-node ~ ➜  kubectl config use-context cluster2
Switched to context "cluster2".

student-node ~ ➜  kubectl get pods -n kube-system  | grep etcd

student-node ~ ✖
Also, there is NO static pod configuration for etcd under the static pod path:

student-node ~ ➜  ssh cluster2-controlplane
Welcome to Ubuntu 23.10 (GNU/Linux 5.4.0-1106-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
Last login: Thu Jul 25 19:31:58 2024 from 192.160.244.4

cluster2-controlplane ~ ➜  ls /etc/kubernetes/manifests/ | grep -i etcd

cluster2-controlplane ~ ✖ 
However, if you inspect the process on the controlplane for cluster2, you will see that that the process for the kube-apiserver is referencing an external etcd datastore:

cluster2-controlplane ~ ✖  ps -ef | grep etcd
root        2906    2515  0 19:26 ?        00:01:17 kube-apiserver --advertise-address=192.160.244.12 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem --etcd-servers=https://192.160.244.3:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root        9228    8883  0 20:01 pts/0    00:00:00 grep etcd
You can see the same information by inspecting the kube-apiserver pod (which runs as a static pod in the kube-system namespace):

cluster2-controlplane ~ ➜  kubectl -n kube-system describe pod kube-apiserver-cluster2-controlplane 
Name:                 kube-apiserver-cluster2-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 cluster2-controlplane/192.160.244.12
Start Time:           Thu, 25 Jul 2024 19:26:33 +0000
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.160.244.12:6443
                      kubernetes.io/config.hash: 469506630571d428e4ddc39d3735a142
                      kubernetes.io/config.mirror: 469506630571d428e4ddc39d3735a142
                      kubernetes.io/config.seen: 2024-07-25T19:26:32.901498317Z
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   192.160.244.12
IPs:
  IP:           192.160.244.12
Controlled By:  Node/cluster2-controlplane
Containers:
  kube-apiserver:
    Container ID:  containerd://00e16e0d46214d1bdd969738966c660eb41a0f3a03ef0d2e0d8c009642381f82
    Image:         registry.k8s.io/kube-apiserver:v1.29.0
    Image ID:      registry.k8s.io/kube-apiserver@sha256:921d9d4cda40bd481283375d39d12b24f51281682ae41f6da47f69cb072643bc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=192.160.244.12
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem
      --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem
      --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem
      --etcd-servers=https://192.160.244.3:2379
--------- End of Snippet---------


by ssh into controlplane of cluster2 and grepping for etcd give me details of the running etcd cluster externally on --etcd-servers=https://192.168.114.246:2379 but no local instance at (127.0.0.1 )
----------------------------------------------------------------------------------------------------------------
How many nodes are part of the ETCD cluster that etcd-server is a part of?-------->

Check the members of the cluster: ETCDCTL_API=3 etcdctl member list ----> run this with all the necessary fields as per these instructions https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster

etcd-server ~ ➜  ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/pki/ca.pem \
  --cert=/etc/etcd/pki/etcd.pem \
  --key=/etc/etcd/pki/etcd-key.pem \
   member list
59ee55985632d394, started, etcd-server, https://192.160.244.3:2380, https://192.160.244.3:2379, false

etcd-server ~ ➜  
This shows that there is only one member in this cluster.

copy etcd backup from one cluster to another-----> scp cluster1-controlplane:/opt/cluster1.db /opt/

----------------------------------------------------------------------------------------------------------------------------

An ETCD backup for cluster2 is stored at /opt/cluster2.db. Use this snapshot file to carryout a restore on cluster2 to a new path /var/lib/etcd-data-new.



Once the restore is complete, ensure that the controlplane components on cluster2 are running.


The snapshot was taken when there were objects created in the critical namespace on cluster2. These objects should be available post restore.


If needed, make sure to set the context to cluster2 (on the student-node):

student-node ~ ➜  kubectl config use-context cluster2
Switched to context "cluster2".

---->
Step 1. Copy the snapshot file from the student-node to the etcd-server. In the example below, we are copying it to the /root directory:

student-node ~  scp /opt/cluster2.db etcd-server:/root
cluster2.db                                                                                                        100% 1108KB 178.5MB/s   00:00    

student-node ~ ➜
Step 2: Restore the snapshot on the cluster2. Since we are restoring directly on the etcd-server, we can use the endpoint https:/127.0.0.1. Use the same certificates that were identified earlier. Make sure to use the data-dir as /var/lib/etcd-data-new:

etcd-server ~ ➜  ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem --key=/etc/etcd/pki/etcd-key.pem snapshot restore /root/cluster2.db --data-dir /var/lib/etcd-data-new
{"level":"info","ts":1721940922.0441437,"caller":"snapshot/v3_snapshot.go:296","msg":"restoring snapshot","path":"/root/cluster2.db","wal-dir":"/var/lib/etcd-data-new/member/wal","data-dir":"/var/lib/etcd-data-new","snap-dir":"/var/lib/etcd-data-new/member/snap"}
{"level":"info","ts":1721940922.060755,"caller":"mvcc/kvstore.go:388","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":951}
{"level":"info","ts":1721940922.0667593,"caller":"membership/cluster.go:392","msg":"added member","cluster-id":"cdf818194e3a8c32","local-member-id":"0","added-peer-id":"8e9e05c52164694d","added-peer-peer-urls":["http://localhost:2380"]}
{"level":"info","ts":1721940922.0732546,"caller":"snapshot/v3_snapshot.go:309","msg":"restored snapshot","path":"/root/cluster2.db","wal-dir":"/var/lib/etcd-data-new/member/wal","data-dir":"/var/lib/etcd-data-new","snap-dir":"/var/lib/etcd-data-new/member/snap"}

etcd-server ~ ➜
Step 3: Update the systemd service unit file for etcd by running vi /etc/systemd/system/etcd.service and add the new value for data-dir:

[Unit]
Description=etcd key-value store
Documentation=https://github.com/etcd-io/etcd
After=network.target

[Service]
User=etcd
Type=notify
ExecStart=/usr/local/bin/etcd \
  --name etcd-server \
  --data-dir=/var/lib/etcd-data-new \
---End of Snippet---
Step 4: make sure the permissions on the new directory is correct (should be owned by etcd user):

etcd-server /var/lib ➜  chown -R etcd:etcd /var/lib/etcd-data-new

etcd-server /var/lib ➜ 


etcd-server /var/lib ➜  ls -ld /var/lib/etcd-data-new/
drwx------ 3 etcd etcd 4096 Jul 15 20:55 /var/lib/etcd-data-new/
etcd-server /var/lib ➜
Step 5: Finally, reload and restart the etcd service.

etcd-server ~ ➜  systemctl daemon-reload 
etcd-server ~ ➜  systemctl restart etcd
etcd-server ~ ➜  
Step 6 (optional): It is recommended to restart controlplane components (e.g. kube-scheduler, kube-controller-manager, kubelet) to ensure that they don't rely on some stale data.
-------------------------------------------------------------------------
Check priority class assigned to system-node-critical use --------> kubectl describe priorityclass system-node-critical

Create a PriorityClass named high-priority, a value of 100000, and a preemption policy of PreemptLowerPriority. Do not set this class as a global default. ---->

To create the class high-priority, create a new yaml file:

vi high-priority.yaml
Add the contents of the yaml file:

apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000
globalDefault: false
description: "This priority class is used for high-priority pods."
preemptionPolicy: PreemptLowerPriority
Save the file using :wq. To create the resource:

kubectl apply -f high-priority.yaml
You should be able to see the new resource if you listed the available priority classes using:

kubectl get priorityclasses

---------------
Now, create another pod in the default namespace named high-prio-pod that runs an nginx image and uses the high-priority PriorityClass.
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: high-prio-pod
  name: high-prio-pod
spec:
  containers:
  - image: nginx
    name: high-prio-pod
  priorityClassName: high-priority


-------------------------
You can compare the priority classes on both pods using the following command: --> kubectl get pods -o custom-columns="NAME:.metadata.name,PRIORITY:.spec.priorityClassName"

--------------------------

To resolve this situation and get the critical-app to a running state, you should:

Assign the high-priority class to the critical-app
Delete and recreate the pod with the new priority
Make sure the pod is in running state after applying the actions.

To update the manifest file of the critical-app pod, save a copy of the manifest file:

kubectl get pod critical-app -o yaml > critical-app.yaml
Edit the saved copy as follows (Output truncated below):

# critical-app.yaml
apiVersion: v1
kind: Pod
metadata:
  ...
  name: critical-app
  ...
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: critical-container
    ...
  dnsPolicy: ClusterFirst
  priorityClassName: high-priority   # Add the high-priority class
  enableServiceLinks: true
  preemptionPolicy: PreemptLowerPriority
  priority: 0  # Remove this line as this is the old default priority
  ...
Save the file using :wq! and then delete the old critical-app pod:

kubectl delete pod critical-app
Apply the new updated manifest file:

kubectl apply -f critical-app.yaml
Ensure the pod is now in running state:

kubectl get pod critical-app


----------------------------------------------
Generate a kubeadm join token

Or copy the one that was generated by kubeadm init command

Join node01 to the cluster using the join token


---------->

To create token:


root@controlplane:~> kubeadm token create --print-join-command
kubeadm join 192.168.114.75:6443 --token aqe7sk.zyr2zpqxa0aomjje --discovery-token-ca-cert-hash sha256:b9fa5652e076b79889b7ceb2f41638be5d6975c2c2970b27ed6c9e601315cd41


next, SSH to the node01 and run the join command on the terminal:

root@node01:~> kubeadm join 192.168.114.75:6443 --token aqe7sk.zyr2zpqxa0aomjje --discovery-token-ca-cert-hash sha256:b9fa5652e076b79889b7ceb2f41638be5d6975c2c2970b27ed6c9e601315cd41
[preflight] Running pre-flight checks
        [WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
[preflight] Reading configuration from the "kubeadm-config" ConfigMap in namespace "kube-system"...
[preflight] Use 'kubeadm init phase upload-config --config your-config.yaml' to re-upload it.
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 1.001734558s
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
-----------------------------------------------------

Security of K8s kube api server -> 1) Flat file with user accounts 2) Using Static token file 3) SSL certificates 4) Identity services like LDAP, Kerberos etc
1--> pwd, username, userid,group(optional. just to assing user to a group) in a csv file need to be up stored and added to --basic-auth=usercredentials.csv in kubeapi-server.service file in ExecStart=.. If its setup with kubeadm tool, modify kubeapi-server pod definition file (yaml file) under command field.
To authenticate with the user from the list, use the curl command --> curl -v -k https//master-node-ip-address:6443/api/v1/pods -u "username1:Password1" along with the payload.
2--> Similar to user credentials, we can use tokens in a csv file and add it to kubeapi-server.service file like --token-auth=user-token-details.csv
For authenticating, use the curl command like ---> curl -v -k https//master-node-ip-address:6443/api/v1/pods --header "Authorization: Bearer KpjCvblaljljadflkjadljj"
Clearly these two ways of authentication is not a recommended approach as its insecure. Also if using kubeadm, use volume mounts to pass auth file.
3) 


Servers in k8s which needs server certificates are 1) kubeapi-server 2) etcd database 3) Kubelet server (to interact with worker nodes, it exposes a https cert and key)
clients access the services. 1) Admin user (admin.crt, admin.key) to auth to kubeapi server 2) scheduler (scheduler.cert, scheduler.key), a client to kubeapiserver to talk to kubeapi server for scheduling pods. it gets kubeapi to schedule pods on right nodes. 3) kube controller manager (controllermanger.cert, controllermanager.key) talks to kubeapi server as well.
4) kubeproxy (kubeproxy.cert, kubeproxy.key) authenticates with kubeapiserver.
Finally CA needs root certificates (CA.cert, CA.key)

--------------------------------------------
Certificate generation. 
--> Use openssl to generate certificate--> openssl genrsa -out ca.key 2048
---> use certificate signing request --> openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA /O=system:masters" -out ca.csr  ----> Certificate signing request is like a certificate with all of your details, but with no signature. In the certificate signing request, we specify the name of the component this certificate is for in the common name or CN field. In this case, since we are creating a certificate for the Kubernetes CA, we name it Kubernetes-CA. In audit log this is the name you see. /O is the parameter used to assign the user under a group system:masters
--> Now sign the certificate --> openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.

for client system components like Kube scheduler, the name should be prefixed with SYSTEM: like 'SYSTEM:KUBE-SCHEDULER'. same for 'SYSTEM:KUBE-CONTROLLER-MANAGER'
Create the rest in similar way.

once certificates are created, use they either by ---> curl https//master-node-ip-address:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt with payload.
or --> use kube-config.yaml and specify the parameters (most kube clients use this method)

ca root certificate copy need to be all the components of k8s.


for kube api server, all the alias names should be in openssl.cnf file as [alt_names] and then use the open ssl req along with -config openssl.cnf
all these certificates go under --client-ca for ca.crt, --tls-cert-file for api server. similarly for etcd and kubelet you need similar ca, key and server certificates.

for kubelets, you need them based on the node like node01, node02etc for each node.

-------------------------
All these certificates, if they are generated manually, they will be at /etc/sytemd/system/kubeapi-serveer.service. if its generated through kubeadm, it will be at /etc/kubernetes/manifests/kube-apiserver.yaml

To decode the certificate run ---> openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout

To inspect logs ----------->> use journalctl -u etccd.service -l if all the setup was done manually.
if setup with kubeadm use ----> kubectl logs etcd-master
if kube api server or etcd is down, use docker container, list them first and run the command for logs --> docker ps -a ---> docker logs containerid
---------------------------

The kube-api server stopped again! Check it out. Inspect the kube-api server logs and identify the root cause and fix the issue.
Run crictl ps -a command to identify the kube-api server container. Run crictl logs container-id command to view the logs.

If we inspect the kube-apiserver container on the controlplane, we can see that it is frequently exiting.

root@controlplane:~# crictl ps -a | grep kube-apiserver
1fb242055cff8       529072250ccc6       About a minute ago   Exited              kube-apiserver            3                   ed2174865a416       kube-apiserver-controlplane
If we now inspect the logs of this exited container, we would see the following errors:

root@controlplane:~# crictl logs --tail=2 1fb242055cff8  
W0916 14:19:44.771920       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: x509: certificate signed by unknown authority". Reconnecting...
E0916 14:19:48.689303       1 run.go:74] "command failed" err="context deadline exceeded"
This indicates an issue with the ETCD CA certificate used by the kube-apiserver. Correct it to use the file /etc/kubernetes/pki/etcd/ca.crt.

Once the YAML file has been saved, wait for the kube-apiserver pod to be Ready. This can take a couple of minutes.

---------------------------------------
* Kubernetes has its own API for certificate signing requests
1) create CertificateSigningRequest object
2) Review the requests
3) Approve the requests with kubectl commands
this certificate can be extracted and sent to user.

----------------------------------------
1) user generates a key (openssl genrsa -out Djiadmin.key 2048
2) sends request to the admin ( openssl req -new -key Djiadmin.key -sub "/CN=Dji" -out Djicsr.csr
3) Admin takes key and creates a cert signing request object using a manifest file. specifically in request field of the yaml file, add the Djicsr.csr file data but not as plain text, add it only after encoding it to base64 format.
4) admins can see the requests by using  ------>> kubectl get csr
5) To approve csr request, -----> kubectl certificate approve Dji
6) to view the approved certificate, -----> kubectl get csr Dji -o yaml
7) ofcourse the certificate which is in base64 needs to be decoded to plain text using the command----> echo "Ls0Q...rtQ=" | base64 --decode
8) This can be shared with the end user.
9) The component which is actully doing these certificate realted operations is ---> Controller manager.CSR APPROVING, CSR SIGNING responsible for these tasks.

------------
example:
Create a CertificateSigningRequest object with the name akshay with the contents of the akshay.csr file
As of kubernetes 1.19, the API to use for CSR is certificates.k8s.io/v1.
Please note that an additional field called signerName should also be added when creating CSR. For client authentication to the API server we will use the built-in signer kubernetes.io/kube-apiserver-client.

Use this command to generate the base64 encoded format as following: -

cat akshay.csr | base64 -w 0   ( remember -w 0 is used for printing it without line spaces)
Finally, save the below YAML in a file and create a CSR name akshay as follows: ----->

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  groups:
  - system:authenticated
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQXBYemJDOVA0MEZBMlRJTk42L0QvN0d1Ujljci9sK3ZEV1NDWHc4TDdjc0ZCCk1TdTgxQ1hGcmkwMDNVSnVpOUY1ckVNL0FYUTkzQVZhSkhncFVHQ0V4MmprUUFvOFhDOVgyandZZXQwZ0F4U1IKUllPZGtwZ0cxSHhFcHVqWjlxQStyelZsa2xpWnFKckxtTHd1dTBiaEVveG9BWjBka21FTURrVlJPN0VHZGVlLwo2MUxZaWpieSt4bGpFTjJOVVVtTDkxRTFXWkx4c0V0VU1lUE40cFlSVXE4Y1dwTkQrWERYanhiMzQva3M1bHlSCnZWdDQreE9LcklDd1llTkZKK0hJNTIwSlhqZ3JRUCthUklFK2ZjdWtDc1pKcTg3WXhMZnFpYjYveUs2aEVzNFEKRzQzbnBGRjY4VXV6a0QrcEUvUVJZb2dJUHBRbXNFVHlTTEFOS1Z4cHJRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBRUtOODBnS3dpT2RWVmdSbExlN2ZCUlRzaEl2ZDhUUlhwMWZCQkJaQ0NhcVpiQm5BOVorCmtZWGJDNFBFaXV2eUxtN2I5QVhRanN6L2wwZDNxbkJpd21LOWhIV3NST0VZbG9CWlhxUmxlaW9UREtFcVljVFUKREFpZnN0NFFDWmJFcnpUWkRDK0F4TnVLS1FjSzJoRFYzVzUvdEdOWFVqYWY5MlJTTGp6VDBKLzF6b1R3dU1JWApya0NiTjhmd2ZZSGdqckxyclBCZTR1RVYzSTNFeHFzZ3lIdDBmSGtsZEFscnI5MlZhbkg3cTBMcEJlK2RsK2hJClpiMWNaNVpWd0hxS3BsK05uc3ZNK2pRZUFpNnVNUnRuV0JFb3RPNGFmQWZaSmlmT3hNa2xXZnR1bENpMHZrMmgKYk1aQksyMW5SR3ZjaDVNeVRQOFY3am1idTFqKzV1b3llc289Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth

---------> kubectl apply -f akshay-csr.yaml
To delete a csr -------> kubectl delete csr agent-smithcsr
-------------------------------------------------------------------------------
To manually authenticate a user use the payload with certificates like--->
curl https://my-kubernetes-hostname: 6443/api/v1/pods \
--key admin.key
--cert admin.crt
--cacert ca.crt

{
"kind": "PodList",
"apiVersion":
"1",
"metadata": {
"selfLink": "/api/v1/pods",
},
"items": []
}

To do this using Kubectl command ----------->
kubectl get pods
--server my-kubernetes-hostname: 6443
--client-key admin.key
--client-certificate admin.crt
--certificate-authority ca.crt
------------------------------------------------

TO do this with Kubeconfig file,

"config" File
--server my-kubernetes-hostname: 6443
--client-key admin.key
--client-certificate admin.crt
--certificate-authority ca.crt

Then run kubectl command like ---> kubectl get pods
--kubeconfig config

**** By default K8s looks for config file in this path. So if we save the config file here we dont need to specify the full path in the config command ----------> $Home/.kube/config

-----------------------------------------
Sections in config file
        - Clusters  ---> Various K8s clusters we need access to
        - Users  --> User accounts which you have access to the clusters. Not createing new users, but using existing users.
        - Contexts --> Context define which users access which cluster.

