kubectl get pods
kubectl get pods -o wide
kubectl describe pod <podname>
kubectl create -f <podcreation.yaml filename> ---------- Delcarative Way
kubectl run mynginx_podname --image=nginx_dockerHubImageName ---- imperative way
kubectl describe pod <pod_name> | grep -c "Containers:"  ---------> to find the number of containers used in the pod
kubectl create -f .\rc-definition.yaml (Create replication Controller)
kubectl delete rc myapp-rc ----- Delete replication controller
kubectl get replicaset
kubectl replace -f replicaset-definition.yaml ( will replace the existing replicaset with any new changes) --- Declarative way
kubectl replace --force -f pod-definition.yaml ( will delete and recreate the pod)

************ your linux version?? ---->  cat /etc/*release*


kubectl scale --replicas=6 -f replicaset-definition.yaml -- Imperative way
kubectl scale --replicas=6 replicaset myapp-replicaset --- Imperative way with type & name format
kubectl edit replicaset myapp-replicaset ------------ Risky.. Imperative way to make changes to the replicaset. Note: Careful.. Saving the edited file will immediately apply the changes.
kubectl delete replicaset myapp-replicaset -- Delete replicaset
kubectl create -f deployment-definition.yaml
kubectl create deployment --image=nginx nginx
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml  ---------> Generate Deployment YAML file (-o yaml). Don’t create it(–dry-run)

kubectl create deployment --replicas=3 --image=httpd:2.4-alpine httpd-frontend --dry-run=client -o yaml > httpd-deployment-definition.yaml  ----> created a file as required

kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml  -----> deployment yaml file with replicas as needed

kubectl get deployments
kubectl get all  ----------------- Shows all the resources created
kubectl get pods,svc ----------------- Shows just pods and services created
kubectl get pods --watch ----------- will show the status of the pods continuously if it changes.
kubectl rollout status deployment/myapp-deployment
kubectl apply -f deployment-definition.yaml  ----------- Make changes to the deployment yaml (like image version) and upgrade
kubectl set image deployment/myapp-deployment \ nginx-container=nginx:1.9.1 ------- upgrading imperatively using container name (only for toubelshooting)

kubectl rollout status deployment.apps/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment

******** Services nodePort range is -- 30K to 32767 ******
-- service uses random algorithm to distribute load across all the pods which it had identified based on the service definition file labels which matched with pods labels from their definition.
kubectl create -f .\service-definition.yaml
kubectl get services
minikube service myapp-service --url -------------------- to get the url of service running on minikube node

kubectl delete pods --all -n default ------- Deltes all the pods running on the namespace by the name 'default'


---------------------namespace ---------------------

kubectl config set-context $(kubectl config current-context) --namespace=dev  --> Permanently switch to a namespace so no namespace mentioned everytime
kubectl get pods --all-namespaces -----> get pods form all namespaces
------------- Troubleshooting -----------------
After fixing the image name, you can re-deploy the changes using below command
kubectl apply -f podCreate.yaml
         or
kubectl edit pod <podName>
Then check the pod details, you should see the STATUS as running

---------------- Terminologies ---------------
replication controller - create multiple pods or nodes to load balance. (older technology and obsolete)
Replica set - new way of implementation (apiversion : apps/v1, selector mandatory)
Poda - apiVersion is v1
Rolling upgrade.
cillium/flannel - used for internal routing, ip address allocation between nodes and pods in a cluster
NodePort (range 30000 to 32767)
ClusterIP
LoadBalancer


--------------------- Binding for Manual scheduling ---------------
create a 'Binding' object (yaml file) to assign a node name to a existing pod and send a post request to the pod's binding api.
Thus mimicking what the actual scheduler does.

If a scheduler is missing, to assign a node manually to a pod, use the field 'nodeName: node01' to the pod definition to get the job done.

---------------- scenarios -------------
kubectl get pods --selector app=App1     -> filters all the pods with the label selector app=App1
 k get pods --selector bu=finance | wc -l  ---> counts the pods along with header
k get pod --selector env=prod,bu=finance,tier=frontend  --> multiple filters

kubectl get all --selector env=prod --no-headers | wc -l  ------> count without headers

kubectl expose pod redis --port=6379 --name redis-service --------> Create a service redis-service to expose the redis application within the cluster on port 6379.

kubectl run httpd --image=httpd:alpine --port=80 --expose ------> Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80. Try to do this with as few steps as possible.


kubectl taint nodes node-name key=value:taint-effect ----> to taint (label) the nodes so pods which do not tolerate (match the labels) the taint will not be placed in the node with the taint value.

Pods which do not tolerate the taint will have 3 response status ---> NoSchedule (pods will not be scheduled on the nodes), PreferNoSchedule (system will try to place the pod on the node but 'no gurantee'), NoExecute (New pods will not be scheduled and any existing pods which do not tolerate the taint will be evicted as well from the node) 

example: kubectl taint nodes node1 app=blue:NoSchedule

Tolerations are added to the pods with tolerations added to spec of the pod yaml defition.
spec:
  tolerations:
    - key: "app"
      operator: "Equal"
      value: "blue"
      effect: "NoSchedule"
*********Note: taint and toleration doesnt mean the tolerated pod always ends up in a tainted node. it can be distributed to untainted node as well. This is just to restrict the specific nodes to have only certain type of pods. To ensure required pods to end up to required nodes, we use a different concept called 'NodeSelector' and 'NodeAffinity' 

kubectl label nodes <node name> <label-key>= <lable-value>  ------> To label a node with 'nodeselctor' property under spec in Pod definition yaml file.
eg: kubecl label nodes node01 size=Large

----------> Run the command: kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule- to untaint the node. ************ Observe the NoSchedule have a '-' at the end to untaint it????

------- Pod lifecycle -- stages
Scheduler places a pod in a node based on these affinity rules
requiredDuringSchedulingIgnoreDuringExecution
preferredDuringScheduingIgnoreDuringExecution

During Scheduing - Pod does not exist and created for the first time
During Execution - This is where the changes is made in the label of the node after the pods are allocated to the node.

Affinity rules are considered and pods are placed on the right nodes when the pods are newly created.
If nodes with matching labels are not available, with requiredDuringSchedulingIgnoreDuringExecution type, the scheduler will mandate that the pod will be placed on a node with given affinity rules. If it cannot find one, the pod will not be scheduled. If placement of pod is crucial, this type will be used. If its not crucial then use preferredDuringScheduingIgnoreDuringExecution, that way it will place it if it cant find the mathing labels.

IgnoreDuringExecution means, already allocated pods doesnt have any change if node lable is made.


preferredDuringScheduingRequiredDuringExecution -- This is when after label change, the pods are evicted from the node if the nodeAffinity rules doesn't match.
--------------------------------------------
kubectl run redis --image=redis --dry-run=client -o yaml > redis-pod.yaml -------->to create a yaml file from 'kubectl run' command is like. Reember --dry-run will not create the resource just the yaml file.

kubectl get replicaset new-replica-set -o yaml > replicaset-definition.yaml

----------------------------------- Expose Vs Service --------------------------------------------

Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
(This will automatically use the pod’s labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
(This will not use the pods labels as selectors, instead, it will assume selectors as app=redis.

You cannot pass in selectors as an option.

So, it does not work very well if your pod has a different label set. So, generate the file and modify the selectors before creating the service)

Create a Service named nginx of type NodePort to expose pod nginx’s port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
(This will automatically use the pod’s labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
(This will not use the pod labels as selectors.)

Both the above commands have their own challenges. While one of them cannot accept a selector, the other cannot accept a node port. I would recommend going with the

kubectl expose
command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.


************ kubectl apply (Declarative way) *****************
Kubernetes nomally takes local config file (stored on our local system),  a live object definition (stored in kubernetes memory)..  on kubernetes and 'last applied configuration'(stored in Live ojbect configuration as an 'annotation' ) before deciding what changes to be made.
If the object doesn't exist, an object is created.
Ideally, an object similar to what we had defined locally is created within kubernetes, but also with additional fields to store status of the object. This is the Live object configuration of the object in kubernetes cluster.
when we do kubectl apply, it does a bit more... the yaml of the local is converted to a json format and then stored as 'last applied configuration' 
For any upgrades, all three objects are compared to understand what changes are to be made to the live object.
If any field is removed from local configuration, the 'last applied config' helps to identify the removed fields and 'live object configuration' is upgraded accordingly.
Note This is only done with kubectl apply(declarative) not 'kubectl create'(imperative) or 'kubectl replace'. They don't store 'last applied configuration' like this.

*********************************************** Static Pod Path ****************
static pods will have nodename appended to them.
another way is to look at its yaml definition and check its 'OwnerReference'. if its a 'replicaSet' then its not a staic pod. if ownerReference is a 'node' its a static pod.


What is the path of the directory holding the static pod definition files? --> Run the command ps -aux | grep kubelet and identify the config file - --config=/var/lib/kubelet/config.yaml. Then check in the config file for staticPodPath. --> found this --config=/var/lib/kubelet/config.yaml --> now do this --> cat /var/lib/kubelet/config.yaml | grep 'staticPodPath'
output is --> staticPodPath: /etc/kubernetes/manifests


ps -ef | grep kube-apiserver | grep admission-plugins -------> Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.

Another way is
kubectl get pods -n kube-system.. get details of kubeapiserver. Then grep using the command
kubectl exec -it kube-apiserver-controlplane -n kube-system --kube-apiserver -h | grep 'enable-admission-plugins'


**************************** webhooks ****************

Create TLS secret webhook-server-tls for secure webhook communication in webhook-demo namespace.

We have already created below cert and key for webhook server which should be used to create secret.

Certificate : /root/keys/webhook-server-tls.crt

Key : /root/keys/webhook-server-tls.key


--> kubectl -n webhook-demo create secret tls webhook-server-tls \
    --cert "/root/keys/webhook-server-tls.crt" \
    --key "/root/keys/webhook-server-tls.key"

************************** Check secuirty context of the pod ******************

controlplane ~ ➜  k get pod pod-with-defaults -o yaml | grep -A2 "securityContext"
  securityContext:
    runAsNonRoot: true
    runAsUser: 1234

*************************************************************************

# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]


---------------->
Deploy a pod with a conflicting securityContext i.e. pod running with a user id of 0 (root)


We have added pod definition file under /root/pod-with-conflict.yaml

Mutating webhook should reject the request as its asking to run as root user without setting runAsNonRoot: false
------ output  -----> controlplane ~ ➜  k create -f pod-with-conflict.yaml 
Error from server: error when creating "pod-with-conflict.yaml": admission webhook "webhook-server.webhook-demo.svc" denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)


****************************** install metrics server with yaml manifest*********************

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

k top nodes -----------> for nodes resource consumption
k top pods ------->for pods resource consumption

k top pods -n default --sort-by='memory'
k top pods -n default --sort-by='cpu'| tail -1 -------> least cpu consuming pod

*************************** inspect logs **************************

k logs webapp-2

kubectl logs <pod_name> -c <container_name>     ---> To inspect logs of a specific container in a pod

-------------------------------- command & Args -------------------------
Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. 

---
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"


*************************** secrets ************
kubectl create secret generic Secret_name --from-literal=<secret_key1>=<secret_value1> \
                                          --from-literal=<secret_key2>=<secret_value2> ------> Imperative way

kubectl create secret generic Secret_name --from-file=<filepath>
eg: kubectl create secret generic app_secret --from-file=app_secret.properties  -----> imperative way

-----------------------
apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
ata:
  ssh-privatekey: UG91cmluZzYlRW1vdGljb24lU2N1YmE=    

kubectl create -f secretSshauth.yaml -------> declarative way

to encrypt, we can use this command in terminal ...> echo -n "Pouring6%Emoticon%Scuba" | base64

echo -n "Pouring6%Emoticon%Scuba" | base64 --decode

to view the secret --> kubectl get secret secret-ssh-auth -o yaml
----------------------------------------------


apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ["sleep2.0"]
      args: ["10"]

-----------------------------------------------------  HPA autoscaler -----------------------------------------------------

YAML file for autoscaler--->
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 80
status:
  currentReplicas: 0
  desiredReplicas: 0

To run the yaml file to autoscale --> kubectl apply -f /root/autoscale.yml ------> Declarative
kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80 --> imperative

------------------------------------------------------------  VPA autoscaler ---------------------------------------------
We have prepared the required YAML files for you to deploy the Vertical Pod Autoscaler (VPA). Simply follow the steps below to apply the necessary configurations:

Step 1: Install VPA Custom Resource Definitions (CRDs)  <<-----------------------------
These CRDs allow Kubernetes to recognize the custom resources that VPA uses to function properly. To install them, run this command:

kubectl apply -f /root/vpa-crds.yml

Step 2: Install VPA Role-Based Access Control (RBAC)   <<---------------------------
RBAC ensures that VPA has the appropriate permissions to operate within your Kubernetes cluster. To install the RBAC settings, run:

kubectl apply -f /root/vpa-rbac.yml

By running these commands, the VPA will be successfully deployed to your cluster, ready to manage and adjust your pod resources dynamically.
------------------------------------------

Clone the VPA Repository and Set Up the Vertical Pod Autoscaler
You are required to clone the Kubernetes Autoscaler repository into the /root directory and set up the Vertical Pod Autoscaler (VPA) by running the provided script.

Steps:
Clone the repository:

First, navigate to the /root directory and clone the repository:

  git clone https://github.com/kubernetes/autoscaler.git

Navigate to the Vertical Pod Autoscaler directory:

After cloning, move into the vertical-pod-autoscaler directory:

   cd autoscaler/vertical-pod-autoscaler

Run the setup script:

Execute the provided script to deploy the Vertical Pod Autoscaler:

   ./hack/vpa-up.sh

By following these steps, the Vertical Pod Autoscaler will be installed and ready to manage pod resources in your Kubernetes cluster.


---------------------------------------------------------------------------
Which of the following are the VPA CRDs that get installed as part of the Vertical Pod Autoscaler setup? --> kubectl get crds | grep verticalpodautoscaler

check Custom Resource Definitions (CRD) --> kubectl get crd

--------------------
VPA deployments in kube-system namespace

k get deploy -n kube-system | grep vpa-*
vpa-admission-controller   1/1     1            1           3m53s
vpa-recommender            1/1     1            1           3m54s
vpa-updater                1/1     1            1           3m54s

check logs for vpa updater -------> k -n kube-system logs vpa-updater-pod_name 
-----------------------------------------------------------------------------------------------

Drain the nodes from a specific node so the pods can be recreated on another node. The node will be cordoned(marked unschedulable). to make them schedulable again, we need to uncordon.

kubectl drain node01

kubectl uncordon node01

kubectl cordon node02 ---> will just mark the node unschedulable but doesn't drain the pods off the node.

-------------------------------------------------------------------------- upgrade clusters --------------------------

To check latest version of components and plan for upgrade using kubeadm tool ---------> kubeadm upgrade plan

Always upgrade controlplane first. Drain all the pods from the controlplane before updating it.

To upgrade Kubernetes version

To seamlessly transition from Kubernetes v1.31 to v1.32 and gain access to the packages specific to the desired Kubernetes minor version, follow these essential steps during the upgrade process. This ensures that your environment is appropriately configured and aligned with the features and improvements introduced in Kubernetes v1.32.

On the controlplane node:

Use any text editor you prefer to open the file that defines the Kubernetes apt repository.

vim /etc/apt/sources.list.d/kubernetes.list
Update the version in the URL to the next available minor release, i.e v1.32.

deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
After making changes, save the file and exit from your text editor. Proceed with the next instruction.

apt update

apt-cache madison kubeadm
Based on the version information displayed by apt-cache madison, it indicates that for Kubernetes version 1.32.0, the available package version is 1.32.0-1.1. Therefore, to install kubeadm for Kubernetes v1.32.0, use the following command:

apt-get install kubeadm=1.32.0-1.1
Run the following command to upgrade the Kubernetes cluster.

kubeadm upgrade plan v1.32.0

kubeadm upgrade apply v1.32.0
Note that the above steps can take a few minutes to complete.

Now, upgrade the Kubelet version. Also, mark the node (in this case, the "controlplane" node) as schedulable.

apt-get install kubelet=1.32.0-1.1
Run the following commands to refresh the systemd configuration and apply changes to the Kubelet service:

systemctl daemon-reload

systemctl restart kubelet

----------------------------------------------------------------------------------------------------------------------

If you are on the controlplane node, run ----------->> ssh node01 to log in to the node01.

-----------------------------------------------------------------------------------------------------------



https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

------------------------------------------------------

WORKING WITH ETCDCTL

etcdctl is a command line client for etcd.

In all our Kubernetes Hands-on labs, the ETCD key-value database is deployed as a static pod on the master. The version used is v3.

To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3.

You can do this by exporting the variable ETCDCTL_API prior to using the etcdctl client. This can be done as follows:

export ETCDCTL_API=3

On the Master Node:

Image

To see all the options for a specific sub-command, make use of the -h or –help flag.

For example, if you want to take a snapshot of etcd, use:

etcdctl snapshot save -h and keep a note of the mandatory global options.

Since our ETCD database is TLS-Enabled, the following options are mandatory:

–cacert               verify certificates of TLS-enabled secure servers using this CA bundle

–cert                  identify secure client using this TLS certificate file

–endpoints=[127.0.0.1:2379] This is the default as ETCD is running on master node and exposed on localhost 2379.

–key                 identify secure client using this TLS key file

For a detailed explanation on how to make use of the etcdctl command line tool and work with the -h flags.
----------------------------------------------------------------------------------------------------------
At what address can you reach the ETCD cluster from the controlplane node?

Check the ETCD Service configuration in the ETCD POD

Use the command kubectl describe pod etcd-controlplane -n kube-system and look for --listen-client-urls

----------------------------use case for etcd backup ----------
The master node in our cluster is planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.


Store the backup file at location /opt/snapshot-pre-boot.db

solution-> use the command to grep for path ------> k -n kube-system describe pod etcd-controlplane | grep '\-\-' (Remember -> Since our ETCD database is TLS-Enabled, the following options are mandatory)


root@controlplane:~# ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db

Snapshot saved at /opt/snapshot-pre-boot.db
root@controlplane:~# 

----------------------------------------------

To restore the stored snapshot -------->

root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db



Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.



Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup).

  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want).

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.



Note 1: As the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run the command: watch "crictl ps | grep etcd" to see when the ETCD pod is restarted.

Note 2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.

Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.



If you do change --data-dir to /var/lib/etcd-from-backup in the ETCD YAML file, make sure that the volumeMounts for etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

-------------------------------------------------------------------------------------------------------------------------------------------

how many clusters are part of the node?------------------> k get-clusters
switch to specific cluster context to get details of that cluster --> k config use-context cluster1


-------------------------------------
If you check out the pods running in the kube-system namespace in cluster2, you will notice that there are NO etcd pods running in this cluster!

student-node ~ ➜  kubectl config use-context cluster2
Switched to context "cluster2".

student-node ~ ➜  kubectl get pods -n kube-system  | grep etcd

student-node ~ ✖
Also, there is NO static pod configuration for etcd under the static pod path:

student-node ~ ➜  ssh cluster2-controlplane
Welcome to Ubuntu 23.10 (GNU/Linux 5.4.0-1106-gcp x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
Last login: Thu Jul 25 19:31:58 2024 from 192.160.244.4

cluster2-controlplane ~ ➜  ls /etc/kubernetes/manifests/ | grep -i etcd

cluster2-controlplane ~ ✖ 
However, if you inspect the process on the controlplane for cluster2, you will see that that the process for the kube-apiserver is referencing an external etcd datastore:

cluster2-controlplane ~ ✖  ps -ef | grep etcd
root        2906    2515  0 19:26 ?        00:01:17 kube-apiserver --advertise-address=192.160.244.12 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem --etcd-servers=https://192.160.244.3:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root        9228    8883  0 20:01 pts/0    00:00:00 grep etcd
You can see the same information by inspecting the kube-apiserver pod (which runs as a static pod in the kube-system namespace):

cluster2-controlplane ~ ➜  kubectl -n kube-system describe pod kube-apiserver-cluster2-controlplane 
Name:                 kube-apiserver-cluster2-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 cluster2-controlplane/192.160.244.12
Start Time:           Thu, 25 Jul 2024 19:26:33 +0000
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.160.244.12:6443
                      kubernetes.io/config.hash: 469506630571d428e4ddc39d3735a142
                      kubernetes.io/config.mirror: 469506630571d428e4ddc39d3735a142
                      kubernetes.io/config.seen: 2024-07-25T19:26:32.901498317Z
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   192.160.244.12
IPs:
  IP:           192.160.244.12
Controlled By:  Node/cluster2-controlplane
Containers:
  kube-apiserver:
    Container ID:  containerd://00e16e0d46214d1bdd969738966c660eb41a0f3a03ef0d2e0d8c009642381f82
    Image:         registry.k8s.io/kube-apiserver:v1.29.0
    Image ID:      registry.k8s.io/kube-apiserver@sha256:921d9d4cda40bd481283375d39d12b24f51281682ae41f6da47f69cb072643bc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=192.160.244.12
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem
      --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem
      --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem
      --etcd-servers=https://192.160.244.3:2379
--------- End of Snippet---------


by ssh into controlplane of cluster2 and grepping for etcd give me details of the running etcd cluster externally on --etcd-servers=https://192.168.114.246:2379 but no local instance at (127.0.0.1 )
----------------------------------------------------------------------------------------------------------------
Check the members of the cluster:

etcd-server ~ ➜  ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/pki/ca.pem \
  --cert=/etc/etcd/pki/etcd.pem \
  --key=/etc/etcd/pki/etcd-key.pem \
   member list
59ee55985632d394, started, etcd-server, https://192.160.244.3:2380, https://192.160.244.3:2379, false

etcd-server ~ ➜  
This shows that there is only one member in this cluster.

