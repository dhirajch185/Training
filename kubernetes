kubectl get pods
kubectl get pods -o wide
kubectl describe pod <podname>
kubectl create -f <podcreation.yaml filename> ---------- Delcarative Way
kubectl run mynginx_podname --image=nginx_dockerHubImageName ---- imperative way
kubectl describe pod <pod_name> | grep -c "Containers:"  ---------> to find the number of containers used in the pod
kubectl create -f .\rc-definition.yaml (Create replication Controller)
kubectl delete rc myapp-rc ----- Delete replication controller
kubectl get replicaset
kubectl replace -f replicaset-definition.yaml ( will replace the existing replicaset with any new changes) --- Declarative way
kubectl scale --replicas=6 -f replicaset-definition.yaml -- Imperative way
kubectl scale --replicas=6 replicaset myapp-replicaset --- Imperative way with type & name format
kubectl edit replicaset myapp-replicaset ------------ Risky.. Imperative way to make changes to the replicaset. Note: Careful.. Saving the edited file will immediately apply the changes.
kubectl delete replicaset myapp-replicaset -- Delete replicaset
kubectl create -f deployment-definition.yaml
kubectl create deployment --image=nginx nginx
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml  ---------> Generate Deployment YAML file (-o yaml). Don’t create it(–dry-run)

kubectl create deployment --replicas=3 --image=httpd:2.4-alpine httpd-frontend --dry-run=client -o yaml > httpd-deployment-definition.yaml  ----> created a file as required

kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml  -----> deployment yaml file with replicas as needed

kubectl get deployments
kubectl get all  ----------------- Shows all the resources created
kubectl get pods,svc ----------------- Shows just pods and services created
kubectl rollout status deployment/myapp-deployment
kubectl apply -f deployment-definition.yaml  ----------- Make changes to the deployment yaml (like image version) and upgrade
kubectl set image deployment/myapp-deployment \ nginx-container=nginx:1.9.1 ------- upgrading imperatively using container name (only for toubelshooting)

kubectl rollout status deployment.apps/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment

******** Services nodePort range is -- 30K to 32767 ******
-- service uses random algorithm to distribute load across all the pods which it had identified based on the service definition file labels which matched with pods labels from their definition.
kubectl create -f .\service-definition.yaml
kubectl get services
minikube service myapp-service --url -------------------- to get the url of service running on minikube node

kubectl delete pods --all -n default ------- Deltes all the pods running on the namespace by the name 'default'


---------------------namespace ---------------------

kubectl config set-context $(kubectl config current-context) --namespace=dev  --> Permanently switch to a namespace so no namespace mentioned everytime
kubectl get pods --all-namespaces -----> get pods form all namespaces
------------- Troubleshooting -----------------
After fixing the image name, you can re-deploy the changes using below command
kubectl apply -f podCreate.yaml
         or
kubectl edit pod <podName>
Then check the pod details, you should see the STATUS as running

---------------- Terminologies ---------------
replication controller - create multiple pods or nodes to load balance. (older technology and obsolete)
Replica set - new way of implementation (apiversion : apps/v1, selector mandatory)
Poda - apiVersion is v1
Rolling upgrade.
cillium/flannel - used for internal routing, ip address allocation between nodes and pods in a cluster
NodePort (range 30000 to 32767)
ClusterIP
LoadBalancer


--------------------- Binding for Manual scheduling ---------------
create a 'Binding' object (yaml file) to assign a node name to a existing pod and send a post request to the pod's binding api.
Thus mimicking what the actual scheduler does.

---------------- scenarios -------------
kubectl get pods --selector app=App1     -> filters all the pods with the label selector app=App1

kubectl expose pod redis --port=6379 --name redis-service --------> Create a service redis-service to expose the redis application within the cluster on port 6379.

kubectl run httpd --image=httpd:alpine --port=80 --expose ------> Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80. Try to do this with as few steps as possible.


kubectl taint nodes node-name key:value:taint-effect ----> to taint (label) the nodes so pods which do not tolerate (match the labels) the taint will not be placed in the node with the taint value.

Pods which do not tolerate the taint will have 3 response status ---> NoSchedule (pods will not be scheduled on the nodes), PreferNoSchedule (system will try to place the pod on the node but 'no gurantee'), NoExecute (New pods will not be scheduled and any existing pods which do not tolerate the taint will be evicted as well from the node) 

example: kubectl taint nodes node1 app=blue:NoSchedule

Tolerations are added to the pods with tolerations added to spec of the pod yaml defition.
spec:
  tolerations:
    - key: "app"
      operator: "Equal"
      value: "blue"
      effect: "NoSchedule"
*********Note: taint and toleration doesnt mean the tolerated pod always ends up in a tainted node. it can be distributed to untainted node as well. This is just to restrict the specific nodes to have only certain type of pods. To ensure required pods to end up to required nodes, we use a different concept called 'NodeSelector' and 'NodeAffinity' 

kubectl label nodes <node name> <label-key>= <lable-value>  ------> To label a node with 'nodeselctor' property under spec in Pod definition yaml file.
eg: kubecl label nodes node-1 size=Large


------- Pod lifecycle -- stages
Scheduler places a pod in a node based on these affinity rules
requiredDuringSchedulingIgnoreDuringExecution
preferredDuringScheduingIgnoreDuringExecution

During Scheduing - Pod does not exist and created for the first time
During Execution - This is where the changes is made in the label of the node after the pods are allocated to the node.

Affinity rules are considered and pods are placed on the right nodes when the pods are newly created.
If nodes with matching labels are not available, with requiredDuringSchedulingIgnoreDuringExecution type, the scheduler will mandate that the pod will be placed on a node with given affinity rules. If it cannot find one, the pod will not be scheduled. If placement of pod is crucial, this type will be used. If its not crucial then use preferredDuringScheduingIgnoreDuringExecution, that way it will place it if it cant find the mathing labels.

IgnoreDuringExecution means, already allocated pods doesnt have any change if node lable is made.


preferredDuringScheduingRequiredDuringExecution -- This is when after label change, the pods are evicted from the node if the nodeAffinity rules doesn't match.
--------------------------------------------
kubectl run redis --image=redis --dry-run=client -o yaml > redis-pod.yaml -------->to create a yaml file from 'kubectl run' command is like. Reember --dry-run will not create the resource just the yaml file.

kubectl get replicaset new-replica-set -o yaml > replicaset-definition.yaml




